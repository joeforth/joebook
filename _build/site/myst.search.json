{"version":"1","records":[{"hierarchy":{"lvl1":"Molecular descriptors and similarity"},"type":"lvl1","url":"/chem-data/descriptors-similarity","position":0},{"hierarchy":{"lvl1":"Molecular descriptors and similarity"},"content":"","type":"content","url":"/chem-data/descriptors-similarity","position":1},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl2":"Molecular descriptors"},"type":"lvl2","url":"/chem-data/descriptors-similarity#molecular-descriptors","position":2},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl2":"Molecular descriptors"},"content":"Molecular descriptors provide a way of encoding information about the structure or properties of a molecule in a primarily numerical form.\n\nRoberto Todeschini and Viviana Consonni who literally wrote several books on molecular descriptors (including \n\nthis one [1] which you can access through UoL library) defined molecular descriptors as follows:\n\n“The molecular descriptor is the final result of a logic and mathematical procedure which transforms chemical information\nencoded within a symbolic representation of a molecule into a useful number, or the result of some standardized experiment.”[2]","type":"content","url":"/chem-data/descriptors-similarity#molecular-descriptors","position":3},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"Calculating descriptors","lvl2":"Molecular descriptors"},"type":"lvl3","url":"/chem-data/descriptors-similarity#calculating-descriptors","position":4},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"Calculating descriptors","lvl2":"Molecular descriptors"},"content":"Molecular descriptors are usually calculated based on a molecular representation [3], like a SMILES string or a molecular graph.\n\nYou have worked with some notation systems to represent molecules, such as SMILES and InChI. The distinction between a molecular representation and some types of molecular descriptors that encode structural information like molecular fingerprints can seem unclear. The table below summarises the information and how it is expressed and used for different types of molecular representations and descriptors.\n\nFeature\n\nString-Based Representation\n\nMolecular Graph (Representation)\n\nMolecular Fingerprint (Descriptor)\n\nProperty Descriptor\n\nDefinition\n\nLinear or hierarchical text notation of molecular structure\n\nGraph-based connectivity model\n\nEncoded numerical feature set\n\nNumerical value representing physical, chemical or biological property\n\nFormat\n\nText string (ASCII)\n\nNodes (atoms) and edges (bonds)\n\nBit vector, integer array\n\nScalar, vector, or matrix\n\nExamples\n\nSMILES (CCO), InChI (InChI=1S/C2H6O/...)\n\nAdjacency matrix, edge list\n\nMACCS keys, ECFP, Morgan FP\n\nLogP, dipole moment, polar surface area (PSA)\n\nTypical use\n\nDatabase searching, input for descriptor calculations or other structure-based methods\n\nInput for descriptor calculations\n\nSimilarity searching, QSAR, clustering\n\nQSAR/QSPR, toxicity prediction, virtual screening\n\nInformation Captured\n\nAtomic connectivity in text format\n\nTopological information, connectivity\n\nSubstructural patterns, fragments\n\nUsually computed, but also experimental molecular properties","type":"content","url":"/chem-data/descriptors-similarity#calculating-descriptors","position":5},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"Molecular descriptors classed by dimension","lvl2":"Molecular descriptors"},"type":"lvl3","url":"/chem-data/descriptors-similarity#molecular-descriptors-classed-by-dimension","position":6},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"Molecular descriptors classed by dimension","lvl2":"Molecular descriptors"},"content":"Descriptors are often described in terms of their dimensionality, which depends on the level of information about the molecule required to generate the descriptor. For example, a 0D descriptor like molecular weight or the number of atoms present can be calculated just from the molecular formula; whereas a 1D property like the number of hydrogen bond donors required some information about the molecule’s connectivity.\n\nDimensionality\n\nWhat It Captures\n\nExamples\n\n0D (Constitutional Descriptors)\n\nAtomic composition only, no bonding info\n\nMolecular weight, atom counts (C, H, O, etc.), molecular formula\n\n1D (Global Properties)\n\nMolecular properties\n\nNumber of rings, LogP, number of H-bond donors\n\n2D (Topological Descriptors)\n\nConnectivity, graph-based structure\n\nWiener Index, Morgan fingerprint, number of rotatable bonds\n\n3D (Geometric & Electronic Descriptors)\n\n3D shape, spatial arrangement\n\nTPSA, dipole moment, molecular volume\n\n4D (Conformational Ensembles)\n\nMultiple conformations, flexible structures\n\nMolecular interaction fields, ensemble pharmacophores\n\nThere are many many different descriptors available. The Todeschini and Consonni book \n\nMolecular Descriptors for Cheminformatics [1] is an encyclopedic reference on descriptors, primarily those relevant in medicinal chemistry or drug discovery-related areas.\n\nOne type of descriptor that finds broad use and has become extremely prevalent due to the rising application of AI and machine learning are molecular fingerprint. This is a very brief introduction to what they are and how they can be used for measuring the similarity of chemical compounds.","type":"content","url":"/chem-data/descriptors-similarity#molecular-descriptors-classed-by-dimension","position":7},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"Molecular fingerprints","lvl2":"Molecular descriptors"},"type":"lvl3","url":"/chem-data/descriptors-similarity#molecular-fingerprints","position":8},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"Molecular fingerprints","lvl2":"Molecular descriptors"},"content":"A molecular fingerprint is a type of descriptor (rather than a representation).\n\nIt is a computed numerical encoding of a molecule’s structure, typically represented as a bit vector or hashed feature vector.\n\nUnlike a molecular graph, which represent connectivity, fingerprints transform structural information into a numerical format for use in QSAR modeling, similarity searching, and machine learning.\n\nFingerprints are derived from molecular structure, making them descriptors rather than raw representations.","type":"content","url":"/chem-data/descriptors-similarity#molecular-fingerprints","position":9},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl4":"Types of Molecular Fingerprints (a non-exhaustive list)","lvl3":"Molecular fingerprints","lvl2":"Molecular descriptors"},"type":"lvl4","url":"/chem-data/descriptors-similarity#types-of-molecular-fingerprints-a-non-exhaustive-list","position":10},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl4":"Types of Molecular Fingerprints (a non-exhaustive list)","lvl3":"Molecular fingerprints","lvl2":"Molecular descriptors"},"content":"Structural (key-based) fingerprints\n\nEncode presence or absence of substructures specified in a pre-defined library.\n\nExample: MACCS keys.\n\n\n\nPath-based fingerprints\n\nCompress all (usually linear) paths in a molecule up to a specified length into a fixed-length bit vector using hashing.\n\nExample: Daylight Fingerprints.\n\nCircular (radial) fingerprints\n\nCapture atomic environments iteratively (e.g., \n\nExtended Connectivity Fingerprint ECFP [4] - nice primer \n\nhere).\n\nExample: Morgan fingerprints - probably the most prevalent molecular fingerprint currently.\n\nPharmacophore-based fingerprints\n\nEncode molecular features relevant to binding (e.g., hydrogen bond donors/acceptors, hydrophobicity) [5].\n\nExample: PH4 fingerprints.\n\nSome other resources to learn about fingerprints\n\nThere is an intro to some of the molecular fingerprints available in the RDKit in the \n\nRDKit book and docs.\n\nThis \n\nRDKit blog post details how to generate various types of fingerprint in the RDKit.\nThe RDKit \n\nGetting Started guide also has info on fingerprints and similarity (see later section), and \n\nunderstanding what bits in the fingerprints mean with some visualisations.","type":"content","url":"/chem-data/descriptors-similarity#types-of-molecular-fingerprints-a-non-exhaustive-list","position":11},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl2":"Molecular Similarity"},"type":"lvl2","url":"/chem-data/descriptors-similarity#molecular-similarity","position":12},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl2":"Molecular Similarity"},"content":"One of the major uses of molecular fingerprints is to assess molecular similarity by comparing the bit vectors (or count vectors) of different molecules.","type":"content","url":"/chem-data/descriptors-similarity#molecular-similarity","position":13},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"How does molecular similarity work with fingerprints?","lvl2":"Molecular Similarity"},"type":"lvl3","url":"/chem-data/descriptors-similarity#how-does-molecular-similarity-work-with-fingerprints","position":14},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"How does molecular similarity work with fingerprints?","lvl2":"Molecular Similarity"},"content":"Generate fingerprints\n\nConvert molecules into fingerprints (e.g., Morgan fingerprints (ECFP), MACCS keys).\n\nCompute a similarity metric to compare fingerprints\n\nThe Tanimoto coefficient (also called Jaccard similarity) is the most common measure used in combination with fingerprints\n\nOther similarity measure include Dice similarity, Cosine similarity, and Euclidean distance (see below)\n\nInterpret similarity scores\n\nDecide what values of score indicates if molecule is similar or different enough.\n\nHow similar is similar? [6] \n\nA Med Chem perspective [7]\n\nHow similar two compounds (or other chemical entities) can be a rather contentious question, but it is at the heart of many areas of discovery and design in chemical and molecular sciences, even in processes as seemingly simple as database searching. Quantitative measures based on encoded representations or fingerprints are therefore essential, but it can be challenging to distil “similarity” down to a single measure.","type":"content","url":"/chem-data/descriptors-similarity#how-does-molecular-similarity-work-with-fingerprints","position":15},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"Similarity measures","lvl2":"Molecular Similarity"},"type":"lvl3","url":"/chem-data/descriptors-similarity#similarity-measures","position":16},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"Similarity measures","lvl2":"Molecular Similarity"},"content":"A variety of similarity measures are used in different applications and fields [8], and the choice of a particular measure might be dependent various factors, such as easy they are to compute, the information being compared and the type of similarity - or difference - they are assessing [7].\n\nMany studies show that similarity coefficient can also show different sensitivities depending on the representation or fingerprint on which they were calculated and the combination of representation/fingerprint and similarity measure might be more or less appropriate in different contexts [9-13].\n\na = number of bits on in fingerprint A (i.e., |A|)\n\nb = number of bits on in fingerprint B (i.e., |B|)\n\nc = number of shared “on” bits between A and B (i.e., |A \\cap B|)\n\nSimilarity Measure\n\nFormula\n\nTanimoto-Jaccard coefficent\n\nS_{AB} = \\frac{c}{a + b - c}\n\nDice coefficent\n\nS_{AB} = \\frac{2c}{a + b}\n\nCosine coefficent\n\nS_{AB} = \\frac{c}{\\sqrt{a b}}\n\nEuclidean distance\n\nD_{AB} = \\sqrt{a + b - 2c}\n\nHamming (Manhattan) distance\n\nD_{AB} = a + b - 2c\n\nTversky coefficent\n\nS_{AB} = \\frac{c}{\\alpha (a - c) + \\beta (b - c) + c}\n\nS_{AB} are similarity metrics; \n\nD_{AB} are distance or dissimilarity measures.","type":"content","url":"/chem-data/descriptors-similarity#similarity-measures","position":17},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl4":"Example applications","lvl3":"Similarity measures","lvl2":"Molecular Similarity"},"type":"lvl4","url":"/chem-data/descriptors-similarity#example-applications","position":18},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl4":"Example applications","lvl3":"Similarity measures","lvl2":"Molecular Similarity"},"content":"Virtual Screening: Find drug-like molecules similar to known active compounds.\n\nClustering: Group similar molecules in chemical databases.\n\nQSAR Modeling: Identify structurally similar molecules with similar biological activity.\n\nDiversity Analysis: Assess chemical diversity in compound libraries.","type":"content","url":"/chem-data/descriptors-similarity#example-applications","position":19},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"Summary","lvl2":"Molecular Similarity"},"type":"lvl3","url":"/chem-data/descriptors-similarity#summary","position":20},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"Summary","lvl2":"Molecular Similarity"},"content":"Molecular descriptors and similarity measures are central to data-driven chemistry, enabling drug design, materials discovery, and predictive modelling.\n\nNo single fingerprint or similarity measure works universally across all fields, necessitating custom approaches for complex systems like crystals [14-16], polymers and soft matter [17, 18]. Graph-based methods and machine learning are key frontiers in tackling these challenges, offering new ways to encode and compare complex chemical structures.","type":"content","url":"/chem-data/descriptors-similarity#summary","position":21},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"For you to consider","lvl2":"Molecular Similarity"},"type":"lvl3","url":"/chem-data/descriptors-similarity#for-you-to-consider","position":22},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"For you to consider","lvl2":"Molecular Similarity"},"content":"How applicable are the kind of molecular descriptors discussed here to inorganic compounds?\n\nWhat kinds of issues might need to be factored into representations or descriptors for extended materials?\n\nAre there particular considerations for polymers vs. crystalline solids?\n\nAre structure and property/activity related in the same way for materials as they are for (for example) bioactive molecules?","type":"content","url":"/chem-data/descriptors-similarity#for-you-to-consider","position":23},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl4":"References","lvl3":"For you to consider","lvl2":"Molecular Similarity"},"type":"lvl4","url":"/chem-data/descriptors-similarity#references","position":24},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl4":"References","lvl3":"For you to consider","lvl2":"Molecular Similarity"},"content":"R. Todeschini and V. Consonni, Molecular Descriptors for Cheminformatics, Wiley-VCH, Weinheim, 2009. \n\nTodeschini & Consonni (2009)\n\nR. Todeschini and V. Consonni, Handbook of Molecular Descriptors, Wiley-VCH, Weinheim, 2000. \n\nTodeschini & Consonni (2000)\n\n1 D. S. Wigh, J. M. Goodman and A. A. Lapkin, A review of molecular representation in the age of machine learning, WIREs Comput Mol Sci, 2022, 12, e1603. \n\nWigh et al. (2022)\n\nD. Rogers and M. Hahn, Extended-Connectivity Fingerprints, J. Chem. Inf. Model., 2010, 50, 742–754. \n\nRogers & Hahn (2010)\n\nM. J. McGregor and S. M. Muskal, Pharmacophore Fingerprinting. 1. Application to QSAR and Focused Library Design, J. Chem. Inf. Comput. Sci., 1999, 39, 569–574. \n\nMcGregor & Muskal (1999)\n\nD. E. Patterson, R. D. Cramer, A. M. Ferguson, R. D. Clark and L. E. Weinberger, Neighborhood Behavior:  A Useful Concept for Validation of “Molecular Diversity” Descriptors, J. Med. Chem., 1996, 39, 3049–3059. \n\nPatterson et al. (1996)\n\nG. Maggiora, M. Vogt, D. Stumpfe and J. Bajorath, Molecular Similarity in Medicinal Chemistry: Miniperspective, J. Med. Chem., 2014, 57, 3186–3204. \n\nMaggiora et al. (2014)\n\n1 P. Willett, J. M. Barnard and G. M. Downs, Chemical Similarity Searching, J. Chem. Inf. Comput. Sci., 1998, 38, 983–996. \n\nWillett et al. (1998)\n\n1 D. Bajusz, A. Rácz and K. Héberger, Why is Tanimoto index an appropriate choice for fingerprint-based similarity calculations?, J. Cheminform., 2015, 7, 20. \n\nBajusz et al. (2015)\n\n1 D. Stumpfe and J. Bajorath, Similarity searching, Wiley Interdiscip. Rev. Comput. Mol. Sci., 2011, 1, 260–282. \n\nStumpfe & Bajorath (2011)\n\n1 J. Bajorath, Molecular crime scene investigation – dusting for fingerprints, Drug Discovery Today: Technologies, 2013, 10, e491–e498. \n\nBajusz et al. (2015)\n\n1 R. Duke, C.-H. Yang, B. Ganapathysubramanian and C. Risko, Evaluating molecular similarity measures: Do similarity measures reflect electronic structure properties?, ChemRxiv, 2025, preprint. \n\nDuke et al. (2025).\n\n1 C. L. Mellor, R. L. Marchese Robinson, R. Benigni, D. Ebbrell, S. J. Enoch, J. W. Firman, J. C. Madden, G. Pawar, C. Yang and M. T. D. Cronin, Molecular fingerprint-derived similarity measures for toxicological read-across: Recommendations for optimal use, Regul. Toxic. Pharmacol., 2019, 101, 121–134. \n\nMellor et al. (2019)\n\n1 A. P. Bartók, R. Kondor and G. Csányi, On representing chemical environments, Phys. Rev. B, 2013, 87, 184115. \n\nBartók et al. (2013).\n\nK. T. Schütt, How to represent crystal structures for machine learning: Towards fast prediction of electronic properties, Phys. Rev. B, 2014, 89 \n\nSchütt (2014).\n\nL. Himanen, M. O. J. Jäger, E. V. Morooka, F. Federici Canova, Y. S. Ranawat, D. Z. Gao, P. Rinke and A. S. Foster, DScribe: Library of descriptors for machine learning in materials science, Computer Physics Communications, 2020, 247, 106949. \n\nHimanen et al. (2020)\n\nS. Stuart, J. Watchorn and F. X. Gu, Sizing up feature descriptors for macromolecular machine learning with polymeric biomaterials, npj Comput Mater, 2023, 9, 1–10. \n\nStuart et al. (2023)\n\nY. Zhao, R. J. Mulder, S. Houshyar and T. C. Le, A review on the application of molecular descriptors and machine learning in polymer design, Polym. Chem., 2023, 14, 3325–3346. \n\nZhao et al. (2023)","type":"content","url":"/chem-data/descriptors-similarity#references","position":25},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints"},"type":"lvl1","url":"/chem-data/fingerprints","position":0},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints"},"content":"Molecular descriptors provide a way of encoding information about the structure or properties of a molecule in a primarily numerical form.\n\nRoberto Todeschini and Viviana Consonni who literally wrote several books on molecular descriptors (including \n\nthis one which you can access through UoL library) defined molecular descriptors as follows:\n\n“The molecular descriptor is the final result of a logic and mathematical procedure which transforms chemical information\nencoded within a symbolic representation of a molecule into a useful number, or the result of some standardized experiment.”[1]\n\nSee Also\n\nSome background on molecular descriptors, fingerprints and similarity","type":"content","url":"/chem-data/fingerprints","position":1},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl3":"Aim of this notebook exercise"},"type":"lvl3","url":"/chem-data/fingerprints#aim-of-this-notebook-exercise","position":2},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl3":"Aim of this notebook exercise"},"content":"This notebook will look at an example of generating and visualising Morgan fingerprints - a 2D descriptor that is type of circular topological fingerprint - for some relatively simple molecules using RDKit. We can then use the fingerprint to assess their similarity using the Tanimoto-Jaccard coefficient.\n\nYou saw how to calculate a Morgan fingerprint when you covered FAIR data in CHEM502, so some of this will be a recap and give you some practice wth RDKit.\n\nNote\n\nTo see the complete notebook, click \n\nhere.\n\n# import statements - make sure to run this cell\nimport requests\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nfrom IPython.display import SVG\n\nfrom rdkit import Chem\nfrom rdkit.Chem import (\n                        AllChem,\n                        rdCoordGen,\n                        Draw,\n                        rdFingerprintGenerator,\n                        PandasTools,\n                        Descriptors\n                        )\n# from rdkit.Chem.Draw import rdMolDraw2D, \n# from rdkit.Chem.Draw import MolsToGridImage\nfrom rdkit.Chem.Draw import IPythonConsole\nfrom rdkit import DataStructs\n\nfrom IPython.display import SVG\nfrom ipywidgets import interact,fixed,IntSlider\n\n\n\n\nIPythonConsole.ipython_useSVG=True\n\n\n\n\n","type":"content","url":"/chem-data/fingerprints#aim-of-this-notebook-exercise","position":3},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl3":"RDKit"},"type":"lvl3","url":"/chem-data/fingerprints#rdkit","position":4},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl3":"RDKit"},"content":"RDKit docs\n\nRDKit [2] is an open-source cheminformatics toolkit used for handling chemical structures, molecular representations and computational chemistry tasks. It includes tools for descriptor calculation, generating molecular fingerprints, similarity searching, virtual screening and QSAR modelling.\n\nOne of RDKit’s key capabilities is computing a wide variety of molecular descriptors. You covered the basics of using RDKit in the FAIR data workshop in CHEM501. There are also some resources below that may be useful, particularly for calculating descriptors in RDKit.\n\nThe RDKit Cookbook\n\nList of available descriptors in the RDKit\n\nCalculating descriptors using RDKit\n\nFingerprints in the RDKit\n\n","type":"content","url":"/chem-data/fingerprints#rdkit","position":5},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl3":"Morgan (circular) fingerprints for some simple-ish example molecules"},"type":"lvl3","url":"/chem-data/fingerprints#morgan-circular-fingerprints-for-some-simple-ish-example-molecules","position":6},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl3":"Morgan (circular) fingerprints for some simple-ish example molecules"},"content":"Molecular fingerprints are an abstract representation of features in a molecule, and can seem very abstract indeed. RDKit provides some methods to visualise how the bits in a fingerprint relate to fragments in the molecule, which can help to understand what the fingerprint actually represents.\n\nThere is a brief intro to Morgan fingerprints in RDKit in the \n\nRDKit Getting Started guide.\n\nIn the cell below, there are some helper functions for generating and visualising the RDKit molecules and fingerprints.\n\n# Helper functions for molecule and fingerprint generation and visualisation.\ndef get_2D_molecule(mol: Chem.Mol, addHs: bool=False) -> Chem.Mol:\n    \"\"\"Get rdkit.Mol with 2D coords\"\"\"\n\n    if addHs:\n        mol = Chem.AddHs(mol) if addHs else mol     \n    rdCoordGen.AddCoords(mol)\n    return mol\n\ndef get_3D_molecule(mol: Chem.Mol) -> Chem.Mol:\n    \"\"\"Get rdkit.Mol with 3D coords\"\"\"\n\n    mol = Chem.AddHs(mol)\n    AllChem.EmbedMolecule(mol)\n    return mol\n\ndef get_molecule_from_smiles(smiles: str, addHs: bool=False, make3D: bool=False) -> Chem.Mol:\n    \"\"\"Get rdkit.Mol from SMILES\"\"\"\n    \n    mol =  Chem.MolFromSmiles(smiles)\n    if make3D:\n        mol = get_3D_molecule(mol)\n    else:\n        mol = get_2D_molecule(mol, addHs=addHs)\n    return mol\n\ndef draw_2D_molecule(mol: Chem.Mol, addLabels: bool=False, forFP: bool=False) -> Draw.MolDraw2DSVG:\n    \"\"\"Draw 2D molecule\"\"\"\n    \n    d2d = Draw.MolToImage(mol)\n    if addLabels:\n        for atom in mol.GetAtoms():\n            # For each atom, set the property \"atomNote\" to a index+1 of the atom\n            if forFP:\n                atom.SetProp(\"atomNote\", str(atom.GetIdx()))\n            else:\n                atom.SetProp(\"atomNote\", str(atom.GetIdx()+1))    \n    return d2d\n\n\ndef get_Morgan_fingerprint_with_bits(mol: Chem.Mol, radius: int=2, fpSize: int=2048) -> tuple:\n    \"\"\"Get Morgan fingerprint and its bit information map\"\"\"\n\n    mfp_gen = rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=fpSize)\n    ao = rdFingerprintGenerator.AdditionalOutput()\n    ao.CollectBitInfoMap()\n    mfp = mfp_gen.GetFingerprint(mol, additionalOutput=ao)\n    bit_info = ao.GetBitInfoMap()\n    return mfp, bit_info \n\ndef get_Morgan_fingerprint_with_ao(mol, radius=2, fpSize=2048):\n    \"\"\"Get Morgan fingerprint and its additional output\"\"\"\n    mfp_gen = rdFingerprintGenerator.GetMorganGenerator(radius=radius,fpSize=fpSize)\n    \n    ao = rdFingerprintGenerator.AdditionalOutput()\n    ao.AllocateAtomCounts()\n    ao.AllocateAtomToBits()\n    ao.AllocateBitInfoMap()\n    \n    mfp = mfp_gen.GetFingerprint(mol, additionalOutput=ao)\n    return mfp, ao\n\ndef draw_Morgan_fps_with_bits(mol: Chem.Mol, mfp, bit_info: dict) -> Draw.MolDraw2DSVG:\n    \"\"\"Get SVG drawer for grid of Morgan fingerprints with bits highlighted\"\"\"\n\n    on_bits_list = [(mol, bit_idx, bit_info, atom_info) for bit_idx in mfp.GetOnBits() for atom_info in range(len(bit_info[bit_idx]))]\n    labels = [f\"Bit {str(i[1])}\" for i in on_bits_list]\n    d = Draw.DrawMorganBits(on_bits_list, molsPerRow=5, legends=labels)  # Draw the on bits\n    return d\n\ndef save_Morgan_fps_with_bits(mol, mfp, bit_info) -> Draw.MolDraw2DSVG:\n    \"\"\"Get drawer of grid of Morgan fingerprints with bits highlighted to save as png\"\"\"\n\n    on_bits_list = [(mol, bit_idx, bit_info, atom_info) for bit_idx in mfp.GetOnBits() for atom_info in range(len(bit_info[bit_idx]))]\n    labels = [f\"Bit {str(i[1])}\" for i in on_bits_list]\n    drawOptions = Draw.rdMolDraw2D.MolDrawOptions()\n    drawOptions.prepareMolsBeforeDrawing = False\n    drawOptions.legendFontSize = 60\n    d = Draw.DrawMorganBits(on_bits_list, molsPerRow=5, legends=labels, subImgSize=(300, 300), useSVG=False, drawOptions=drawOptions)  # Draw the on bits\n    return d\n\n\ndef renderFpBit(mol, bit_idx, bit_info,fn):\n    return (display(fn(mol, bit_idx, bit_info)))\n\ndef draw_interactive_Morgan_fps_with_bits(mol, bit_info):\n    \"\"\"Draw Morgan fingerprints with bits highlighted with interactive bit selection\"\"\"\n\n    interact(renderFpBit, bit_idx=list(bit_info.keys()),mol=fixed(mol),\n            bit_info=fixed(bit_info),fn=fixed(Draw.DrawMorganBit));\n\n\n\n","type":"content","url":"/chem-data/fingerprints#morgan-circular-fingerprints-for-some-simple-ish-example-molecules","position":7},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl4":"Find SMILES for some common compounds","lvl3":"Morgan (circular) fingerprints for some simple-ish example molecules"},"type":"lvl4","url":"/chem-data/fingerprints#find-smiles-for-some-common-compounds","position":8},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl4":"Find SMILES for some common compounds","lvl3":"Morgan (circular) fingerprints for some simple-ish example molecules"},"content":"To start with, we will acquire the SMILES strings of some well known compounds. In case you need a SMILES recap: \n\nDaylight theory, \n\nquick tutorial.\n\nThe \n\nChemical Identifier Resolver (CIR) service is run by the CADD Group at the NCI/NIH as part of their \n\nCactus server.\n\nIn addition to its \n\nweb interface, it has an easy-to-use URL API: You can supply a chemical identifier and requests that it returns a representation of a specified type as a string. As well as allowing searches by various molecular representations, you can also search by and for IUPAC Names and even non-standard names. WYou can see the in/output formats it can handle on the web interface.\n\nCactus API urls have the form: https://cactus.nci.nih.gov/chemical/structure/<structure identifier>/<representation> so we can use the requests library to programmatically access representations for compounds.\n\nYou used requests for API access in CHEM502. Check the \n\nquickstart guide for a reminder.\n\nROOT_URL = \"https://cactus.nci.nih.gov/chemical/structure/\"\n\n# For example, to retrieve the InChIKey for the structure with the SMILES \"c1ccccc1\", we would use the following URL:\nidentifier = \"c1ccccc1\"\nrepresentation = \"stdinchikey\"\n\nquery_url = f\"{ROOT_URL}{identifier}/{representation}\"\n\nresponse = requests.get(query_url)\n\nif response:\n    print(response.text)\nelse:\n    raise Exception(f\"Cactus request failed: {response.status_code}\")\n\n\n\n\n\n# Let's try getting a SMILES representation based on a common, non-systematic name for a compound.\n\nidentifier = \"aspirin\"\nrepresentation = \"smiles\"\n\nquery_url = f\"{ROOT_URL}{identifier}/{representation}\"\n\nresponse = requests.get(query_url)\nif response:\n    print(response.text)\nelse:\n    raise Exception(f\"Cactus request failed: {response.status_code}\")\n\n\n\n\n\n# We can create an RDKit molecule from the SMILES string and visualise to check it is actually aspirin.\n# In 2D, ChemDraw graph style:\n\naspirin = Chem.MolFromSmiles(response.text)\naspirin\n\n\n\n# And a 3D view:\nIPythonConsole.drawMol3D(aspirin)\n\n\n\nDoes it agree with what Wikipedia thinks aspirin looks like?\n\nAspirin on wiki\n\nIt looks like that is the correct SMILES for aspirin.\n\n# Here is a function so the process of getting the SMILES can be repeated for multiple compounds.\n# It includes a sleep time (`time.sleep`) to avoid overloading the server.\n\ndef get_smiles_from_name(name):\n\n    # TODO: Write a function that retrieves the SMILES string using the Cactus API\n    # to the CIR service for a compound based on its common name and returns the SMILES\n    # string as text.\n\n    pass\n\n\n\n\n\ncompounds = [\"epinephrine\", \"ibuprofen\", \"dopamine\", \"caffeine\", \"naproxen\", \"paracetamol\", \"paraxanthine\", \"vanillin\",\n             \"adenosine\", \"aspirin\", \"niacinamide\", \"theobromine\", \"diclofenac\", \"theophylline\", \"amphetamine\"]\n\ncompounds_smiles = {compound: get_smiles_from_name(compound) for compound in compounds}\ncompounds_smiles\n\n\n\n# Visualise the molecules\n\nmols = [Chem.MolFromSmiles(smiles) for smiles in compounds_smiles.values()]\nDraw.MolsToGridImage(mols, molsPerRow=5, legends=compounds_smiles.keys())\n\n\n\n","type":"content","url":"/chem-data/fingerprints#find-smiles-for-some-common-compounds","position":9},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl3":"Generating a Morgan fingerprint"},"type":"lvl3","url":"/chem-data/fingerprints#generating-a-morgan-fingerprint","position":10},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl3":"Generating a Morgan fingerprint"},"content":"We will use niacinamide to see an example of generating its Morgan fingerprint and looking at the information encoding in its bit vector.\n\nniacinamide = get_molecule_from_smiles(compounds_smiles[\"niacinamide\"])\nniacinamide\n\n\n\nThe Morgan fingerprint (MFP) is a type of circular fingerprint that encodes the presence of substructures in the molecule.\n\nThe fingerprint can be obtained as a bit vector (a binary array of 0s and 1s) or as a set of counts.\n\nIn the bit vector, an ‘on’ bit, i.e. a “1”, indicates the presence of a particular substructure in the molecule.\n\nThe count-based fingerprint also stores the frequency that the substructure occurs.\n\nHere we will see the fingerprint as a bit vector. You can see the function used to generate the MFP further up in the notebook. It uses an RDKit \n\nMorgan fingerprint generator to create a fingerprint of a molecule.\n\nThe function then returns the fingerprint and a bit information map that can be used to highlight the substructures in the molecule that correspond to the bits in the fingerprint.\n\nniacinamide_mfp, niacinamide_bi = get_Morgan_fingerprint_with_bits(niacinamide)\ndisplay(type(niacinamide_mfp), type(niacinamide_bi))\n\n\n\n\n\n# Display the fingerprint in the form of a bit matrix (the array of 2048 bits is split into 16 rows of 128 bits just to dispaly more easily)\n\nbits = np.array(niacinamide_mfp).reshape(16,128)\nfor row in bits:\n    print(*row)\n\n# show the number of bits that are on\nprint(f\"Number of on bits (value is 1): {niacinamide_mfp.GetNumOnBits()}\")\n\n\n\n","type":"content","url":"/chem-data/fingerprints#generating-a-morgan-fingerprint","position":11},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl3":"Interpreting bit information"},"type":"lvl3","url":"/chem-data/fingerprints#interpreting-bit-information","position":12},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl3":"Interpreting bit information"},"content":"The bit information map is a dictionary where the keys are the bit indices and the values are sets of pair tuples describing the fragment that contribute to that bit.\n\nThe pair tuples are of the form (atomId, radius) where atomId is the index of central atom and the radius is the extent of the circle (number of bonds from the central atom) covering the fragment.\n\ndisplay(draw_2D_molecule(niacinamide, forFP=True, addLabels=True))\n\nniacinamide_bi\n\n\n\nFrom the niacinamide MFP, the first entry in bit info dictionary is for bit 140:140: ((0, 1),)\n\nStarting at the atom at index 0 - the diagram shows this is the primary amine nitrogen - and drawing a circle that encloses atoms one bond away locates a pattern in the molecule that matches whatever substructure is represented by bit 140.\n\nYou can see there can be some bits where more than atom’s environment also contributes to the same bit (bit 1873 for niacinamide). This is a \n\nbit collision and can cause some information loss which could affect further computations based on the fingerprint - particularly relevant for machine learning performance - if the collisions are extensive.\n\nMaking the fingerprint larger can help to reduce bit collisions, but comes with computational expense. Using a count-based MFP rather than the bit vector means that information about the number of number of atom environments for a bit are retained, so count-based MFPs may be preferred if the fingerprint will be used in ML.\n\n","type":"content","url":"/chem-data/fingerprints#interpreting-bit-information","position":13},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl4":"Visualising bits in the Morgan fingerprint","lvl3":"Interpreting bit information"},"type":"lvl4","url":"/chem-data/fingerprints#visualising-bits-in-the-morgan-fingerprint","position":14},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl4":"Visualising bits in the Morgan fingerprint","lvl3":"Interpreting bit information"},"content":"The parts of the molecule contributing to the ‘on’ bits (bits with a value of 1) can also be visualised.\n\nThe colors of the Morgan bits’ highlights indicate the nature of the atoms in the neighbouring environment of the central atom. The radius for this fingerprint was set to 2, so it considers the local environment of the central atom up to atoms two bonds away.\n\nBlue - central atom in the environment\n\nYellow - aromatic atoms\n\nGrey - aliphatic ring atoms\n\n* + faint grey bonds - an atom not part of the bit, but shows the connectivity of atoms that are.\n\n# Use rdkit to draw the fragments corresponding to the on bits in the bit information map\ni = draw_Morgan_fps_with_bits(niacinamide, niacinamide_mfp, niacinamide_bi)\ni\n\n\n\nThis makes it striaghtforward to infer that bit 1873 encodes the presence of unsubtituted aromatic carbon. There may be additional criteria, but carbons 4, 5, 6, 8 all match the substructure.\n\n# img = save_Morgan_fps_with_bits(niacinamide, niacinamide_mfp, niacinamide_bi)\n# img.save(\"niacinamide_mfp.png\")\n\n\n\nShort notebook fanciness diversion\n\nYou can use ipywidgets to create a visualisation of the bits in a notebook that lets you select which bit you want to view:\n\ndraw_interactive_Morgan_fps_with_bits(niacinamide, niacinamide_bi)\n\n\n\n","type":"content","url":"/chem-data/fingerprints#visualising-bits-in-the-morgan-fingerprint","position":15},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl2":"Molecular Similarity"},"type":"lvl2","url":"/chem-data/fingerprints#molecular-similarity","position":16},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl2":"Molecular Similarity"},"content":"One of the major uses of molecular fingerprints is to assess molecular similarity by comparing the bit vectors (or count vectors) of different molecules.\n\nWe can generate fingerprints for our small collection of molecules, then using one as a reference, calculate a measure of how similar the other molecules are to that molecule.\n\nTo start with, we will use caffeine as the reference and locate which of the other compounds are most similar.\n\n# A quick reminder of the compounds we are working with:\n\nDraw.MolsToGridImage(mols, molsPerRow=5, legends=compounds_smiles.keys())\n\n\n\n\nThe RDKit has a module for working with molecules in Pandas, which can be particularly useful if you are dealing with a significant amount of molecules or data.\n\nPandasTools module\n\n# create a pandas DataFrame with the name and SMILES for each compound\n\nmols_df = pd.DataFrame.from_dict(compounds_smiles, orient=\"index\", columns=[\"SMILES\"]).reset_index().rename(columns={\"index\": \"name\"})\nmols_df\n\n\n\n# We could use the existing list of RDKit molecules to add a column to the DataFrame, but we will use the SMILES strings \n# to generate the molecules again to show how to use the `PandasTools` module. The \"ROMol\" column will contain the RDKit molecules.\n\nPandasTools.AddMoleculeColumnToFrame(mols_df, smilesCol=\"SMILES\")\nmols_df\n\n\n\n# We can then add the Morgan fingerprints to the DataFrame.\n\nmfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)\n\nmols_df[\"Morgan_fingerprint\"] = mols_df[\"ROMol\"].map(lambda x: mfpgen.GetFingerprint(x))\nmols_df\n\n\n\n# Finally, we can calculate the Tanimoto similarity between the caffeine molecule and each of the other molecules in the DataFrame.\n\nmols_df[\"Tanimoto_similarity_caffeine\"] = mols_df[\"Morgan_fingerprint\"].map(lambda x: DataStructs.TanimotoSimilarity(mols_df[\"Morgan_fingerprint\"][3], x))\n\nmols_df\n\n\n\n# To see which molecule is most similar to caffeine, we can sort the DataFrame by the Tanimoto similarity.\n\nmols_df.sort_values(\"Tanimoto_similarity_caffeine\", ascending=False)\n\n\n\nThe Tanimoto similarity coefficient shows that theobromine, paraxanthine and theophylline are clearly much more similar to caffeine than the other compounds in the set.\n\nIs this what you expect just from considering the molecular structures?\n\nDo you know anything else about this set of molecules that explains why they are quite closely related?","type":"content","url":"/chem-data/fingerprints#molecular-similarity","position":17},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl4":"For you to try:","lvl2":"Molecular Similarity"},"type":"lvl4","url":"/chem-data/fingerprints#for-you-to-try","position":18},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl4":"For you to try:","lvl2":"Molecular Similarity"},"content":"Try changing the radius over which the Morgan fingerprint is calculated (the value is passed when you create the fingerprint generator). What difference does this make to the Tanimoto score calculated for the new fingerprint? Can you explain why?\n\nTry generating a different type of fingerprint, e.g. an RDKit fingerprint, and see how different the Tanimoto coefficients for that type of fingerprint are.\n\nRDKit has drawing tools to draw \n\nSimilarity Maps. See if you can generate a map to compare caffeine with one of the related compounds.\n\nChoose a different compound as the reference and to identify any other groups of related molecules in the set.\n\n","type":"content","url":"/chem-data/fingerprints#for-you-to-try","position":19},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl4":"References","lvl2":"Molecular Similarity"},"type":"lvl4","url":"/chem-data/fingerprints#references","position":20},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl4":"References","lvl2":"Molecular Similarity"},"content":"R. Todeschini and V. Consonni, Handbook of Molecular Descriptors, Wiley-VCH, Weinheim, 2000. \n\nTodeschini & Consonni (2000)\n\nRDKit: Open-source cheminformatics. \n\nhttps://​www​.rdkit​.org\n\n","type":"content","url":"/chem-data/fingerprints#references","position":21},{"hierarchy":{"lvl1":"Least squares optimisation"},"type":"lvl1","url":"/chem-data/least-squares-opt","position":0},{"hierarchy":{"lvl1":"Least squares optimisation"},"content":"Least squares optimisation is used extensively in chemistry and other fields to find best-fit solutions for modelling datasets.\n\nGiven a function to model observed data, least squares methods find the best-fit solution by adjusting the model’s parameters to minimise its error. This is done by reducing the total sum of the squared differences (the residuals) between the model’s predictions and the data.\n\nTo get an idea of what is actually going on, we can look a simple case of fitting a straight line through a set of points n defined by their x and y values (x_n, y_n). These could represent set of experimental observations like the yield of a reaction as a function of reaction temperature.","type":"content","url":"/chem-data/least-squares-opt","position":1},{"hierarchy":{"lvl1":"Least squares optimisation","lvl2":"1. Fitting a Line Through Two Points as a System of Equations"},"type":"lvl2","url":"/chem-data/least-squares-opt#id-1-fitting-a-line-through-two-points-as-a-system-of-equations","position":2},{"hierarchy":{"lvl1":"Least squares optimisation","lvl2":"1. Fitting a Line Through Two Points as a System of Equations"},"content":"Let’s say we observe two data points (x_1, y_1) and (x_2, y_2).\n\nWe will assume the data follows a straight-line model:y = mx + c\n\nFor the two points, this gives two equations:m x_1 + c = y_1m x_2 + c = y_2\n\nThis is simply a system of two linear equations with two unknowns (m and c), which we can solve exactly: We use standard algebra to solve the simultaneous equations to find m and c.","type":"content","url":"/chem-data/least-squares-opt#id-1-fitting-a-line-through-two-points-as-a-system-of-equations","position":3},{"hierarchy":{"lvl1":"Least squares optimisation","lvl2":"2. Fitting a Line Through Two Points in Matrix Form"},"type":"lvl2","url":"/chem-data/least-squares-opt#id-2-fitting-a-line-through-two-points-in-matrix-form","position":4},{"hierarchy":{"lvl1":"Least squares optimisation","lvl2":"2. Fitting a Line Through Two Points in Matrix Form"},"content":"We can rewrite the same system in matrix notation:\\begin{bmatrix} x_1 & 1 \\\\ x_2 & 1 \\end{bmatrix}  \n\\begin{bmatrix} m \\\\ c \\end{bmatrix}  \n=  \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix}\n\nor more generally:A x = b\n\nwhere:A = \\begin{bmatrix} x_1 & 1 \\\\ x_2 & 1 \\end{bmatrix}, \\quad\nx = \\begin{bmatrix} m \\\\ c \\end{bmatrix}, \\quad\nb = \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix}\n\nSince we have exactly two equations for two unknowns, we can solve this directly using algebra (e.g. by inverting the matrix A assuming it is non-singular).","type":"content","url":"/chem-data/least-squares-opt#id-2-fitting-a-line-through-two-points-in-matrix-form","position":5},{"hierarchy":{"lvl1":"Least squares optimisation","lvl2":"3. What If There Are More Than Two Points?"},"type":"lvl2","url":"/chem-data/least-squares-opt#id-3-what-if-there-are-more-than-two-points","position":6},{"hierarchy":{"lvl1":"Least squares optimisation","lvl2":"3. What If There Are More Than Two Points?"},"content":"Now, suppose we have more than two data points (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n).\n\nWe still use the same setup for the matrices:\\begin{bmatrix} x_1 & 1 \\\\ x_2 & 1 \\\\ \\vdots & \\vdots \\\\ x_n & 1 \\end{bmatrix}  \n\\begin{bmatrix} m \\\\ c \\end{bmatrix}  \n=  \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\n\nBut now there are more equations than unknowns (this is known as an overdetermined system). It means that there is no exact solution.\n\nImagine you have done this reaction at 20 different temperatures and worked out the yield for each one. You are trying to find the line of best fit through those points. There will always be some distance between at least some of those points and whatever line you have drawn, even if the straight line model is the best representation of the relationship.\n\nInstead of finding an exact solution, we look for the best approximate solution by minimising the error between the predicted values and the observed data.\n\nSo instead solving Ax = b exactly, least squares methods do this by trying to solveA^T A x = A^T b\n\nThis equation gives the values of x (containing the parameters of our model, m and c) that minimise the error, as quantified by the sum of squared residuals (SSR) (the difference between the model and the observed data).","type":"content","url":"/chem-data/least-squares-opt#id-3-what-if-there-are-more-than-two-points","position":7},{"hierarchy":{"lvl1":"Least squares optimisation","lvl4":"Direct methods","lvl2":"3. What If There Are More Than Two Points?"},"type":"lvl4","url":"/chem-data/least-squares-opt#direct-methods","position":8},{"hierarchy":{"lvl1":"Least squares optimisation","lvl4":"Direct methods","lvl2":"3. What If There Are More Than Two Points?"},"content":"For some problems, it is possible to solve this equation directly.\n\nIf A^T A is invertible and it is computationally feasible, you can solve for x directly \n\nusing algebra, which is the approach used by \n\nOrdinary Least Squares (OLS). This would give us the best-fit values for m and c.\n\nOther direct methods for solving least squares problems include \n\nSingular Value Decomposition (SVD), which can improve stability when A^T A is ill conditioned. SVD also has advantages when working with noisy data, offers greater computationally efficiency and can enhance the interpretability of the results.","type":"content","url":"/chem-data/least-squares-opt#direct-methods","position":9},{"hierarchy":{"lvl1":"Least squares optimisation","lvl2":"4. Iterative Methods for Least Squares Optimisation"},"type":"lvl2","url":"/chem-data/least-squares-opt#id-4-iterative-methods-for-least-squares-optimisation","position":10},{"hierarchy":{"lvl1":"Least squares optimisation","lvl2":"4. Iterative Methods for Least Squares Optimisation"},"content":"For some minimisation problems, computing the inverse of A^T A, i.e. (A^T A)^{-1}, can be computationally expensive (e.g. for very large datasets) or A^T A is ill-conditioned (close to singular, making it unstable to inversion). In these cases, an iterative approach can be used instead.\n\nIterative methods - as the name suggests - adjust the value of x step-by-step gradually minimising the sum of squared residuals until a minimum is located.\n\nSteepest Descent adjusts x using the gradient of the squared error. The algorithm moves in the direction largest downward - most negative - gradient to minimise the error. While effective, it can be inefficient due to slow convergence compared to other iterative methods. (“Steepest descent” and “gradient descent” are often used interchangably in some areas. Strictly, they are different as steepest descent is a special case of gradient descent. I’d suggest asking a mathematician, or ChatGPT failing that.)\n\nLevenberg-Marquardt is a more advanced technique that dynamically adjusts the step size depending on how close it is to a solution. This means it offers faster convergence than Steepest Descent. It is commonly used for nonlinear curve fitting and is the default method for least squares fitting in scipy.optimize.curve_fit due to its fast and stable performance.","type":"content","url":"/chem-data/least-squares-opt#id-4-iterative-methods-for-least-squares-optimisation","position":11},{"hierarchy":{"lvl1":"Least squares optimisation","lvl3":"Key points","lvl2":"4. Iterative Methods for Least Squares Optimisation"},"type":"lvl3","url":"/chem-data/least-squares-opt#key-points","position":12},{"hierarchy":{"lvl1":"Least squares optimisation","lvl3":"Key points","lvl2":"4. Iterative Methods for Least Squares Optimisation"},"content":"Fitting a line through two points is just solving two simultaneous equations.\n\nFitting a line through many points leads to an overdetermined system, where we use least squares to find the best fit.\n\nLeast squares methods find an approximate solution that minimises the error between the predictions and the observed data.\n\nDirect methods such as OLS and SVD solve a set of equations that mathematically express the least squares condition, providing the best-fit parameters in a single calculation.\n\nIterative methods offer an alternative approach by making step-by-step adjustments to the solution until the error is minimised.\n\n","type":"content","url":"/chem-data/least-squares-opt#key-points","position":13},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)"},"type":"lvl1","url":"/chem-data/spectrum-analysis","position":0},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)"},"content":"Experimental characterisation methods like NMR and IR commonly produce datasets in the form of 1-dimensional (or sometimes 2D) spectra. For simple 1D NMR experiments, for example, the spectrum is usually output in the form of intensity vs. chemical shift \\delta in ppm.","type":"content","url":"/chem-data/spectrum-analysis","position":1},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Analysing spectra"},"type":"lvl3","url":"/chem-data/spectrum-analysis#analysing-spectra","position":2},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Analysing spectra"},"content":"Determining information about the structure of molecules in the sample usually starts with extracting information about the peaks present in the NMR spectrum. The peak information can then be interpreted to provide specific information about the composition and connectivity of the molecule.\n\nIn general, fitting peaks with a function that models the shape serves several purposes including: to gain some fundamental understanding of the nature of the data (e.g. does the method and/or specific instrument produce a particular peak shape); if the peaks are modelled well, the information on their centres, amplitudes can be more precise.","type":"content","url":"/chem-data/spectrum-analysis#analysing-spectra","position":3},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Aim of this notebook exercise"},"type":"lvl3","url":"/chem-data/spectrum-analysis#aim-of-this-notebook-exercise","position":4},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Aim of this notebook exercise"},"content":"To get an overview how 1D spectral data can be analysed to acquire information about the peaks - positions of the centres, widths, intensities, etc. - we will look at a simple NMR spectrum and practise some pandas, matplotlib and scipy skills along the way.\n\nProcessing spectra involves modelling data. This exercise also gives us a chance to look more closely at how a model is being applied to the data and see some of the considerations and issues you should be aware of as you work with even simple data problems.\n\nNote\n\nTo see the complete notebook, click \n\nhere.\n\n# import statements\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.signal import find_peaks, peak_widths\nfrom scipy.optimize import curve_fit\n\n\n\nThe 13C NMR spectrum of ethanol is stored as a csv file (13C_EtOH.csv) in the data directory data.\n\nIt is a good idea to check the contents of a file before you try to load it using Python.\n\nThis is particularly the case with data files to get an idea of the structure of the file, whether it has any header lines that you might want to skip over or if the column names are present, for example.\n\nTip\n\nYou can run shell (terminal) command from a Jupyter notebook. More info here: \n\nhttps://​tinyurl​.com​/2yv37k2x\n\n# Use the `head` shell command to look at the first few lines of the file \n\n\n\n\n\n# Read the NMR spectrum from the csv into a pandas dataframe called nmr_spec\n\n# nmr_spec = ...\n\n\n\n\n# Uncomment line below and run cell to complete code to load the csv file\n# %load ../code_snips/load_nmr_csv.txt\n\n\n\n\n\n# Plot the spectrum\nfig, ax = plt.subplots()\nax.plot(nmr_spec[\"ppm\"], nmr_spec[\"intensity\"])\n\nplt.xlabel(\"Chemical shift $\\\\delta$ / ppm\")\nplt.ylabel(\"Intensity\")\n\nplt.xlim((70, 0))\nplt.title(r\"$\\mathregular{^{13}C}$ NMR spectrum of ethanol\")\nplt.show()\n\n\n\nThe code above uses matplotlib’s pyplot.plot to graph the spectrum. This might be how you have mostly seen plotting in matplotlib so far.\n\nPandas also provides a \n\nplot method on its DataFrame and Series objects that offers a convenient way to \n\nvisualise data using matplotlib (it uses matplotlib by default but can be changed to others, e.g. plotly).\n\nFor example, it will pick up axis labels from the DataFrame columns, but you can also modify if preferred.\n\n# Try it yourself:\n# Plot an equivalent graph of the NMR spectrum using DataFrame.plot()\n\n# Your code here...\n\n\n\n# Uncomment the line below and run the cell to see some code that works\n# %load ../code_snips/df_plot.txt\n\n\n\nDecomposing the spectrum into a set of peaks can sometimes be incorporated into the processing that is done by the experimental acquisition software. There are also various Python packages dedicated to different types of spectroscopic data which can facilitate integration into automated data processing pipelines.\n\nTo get a picture of what is going in this process, we can use some of the general methods available in scipy’s signal processing and optimisation subpackages to analyse the peaks in our simple NMR spectrum.\n\n","type":"content","url":"/chem-data/spectrum-analysis#aim-of-this-notebook-exercise","position":5},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl2":"Peak processing in scipy"},"type":"lvl2","url":"/chem-data/spectrum-analysis#peak-processing-in-scipy","position":6},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl2":"Peak processing in scipy"},"content":"In CHEM501, you used some of the functions available in \n\nSciPy’s \n\noptimize subpackage that can be used to fit population distributions by modelling the population distribution function (PDF) as a curve that follows a Gaussian (or Lorentzian) function.\n\nWe can use the same process to fit peaks in experimentally measured datasets like NMR spectra.\n\nThe peaks in NMR spectra are usually described as Lorentzian functions, but sometimes Gaussian or pseudo-Voigt (a mixture of Gaussian and Lorentzian) shapes are used. For our 13C NMR spectrum, we can define the Lorentzian and Gaussian functions:\n\nLorentziany = \\frac{A}{\\pi} \\frac{W/2}{(x-x_0)^2+(W/2)^2}\n\nwhere A is the amplitude of the peak, W is the full width at half maximum (FWHM) and x0 is value of x at the peak centre.\n\nGaussiany = A \\cdot \\frac{1}{\\sigma\\sqrt{2\\pi}}\\;\\exp(-\\frac{(x-x_0)^2}{2\\sigma^2})-$$y = f(x) = A \\cdot \\frac{1}{\\sigma\\sqrt{2\\pi}}\\;e^{-\\frac{(x-x_0)^2}{2\\sigma^2}}$$-\n\nHere, x0 rather than \\mu is used to represent the centre of the peak (for the normal PDF, \\mu was the mean of the  distribution) and \\sigma and the FWHM, W, of the peak are related by:W = \\sigma\\sqrt{8\\:\\ln2}\n\n# These functions will calculate a peak using a Gaussian or Lorentzian function as defined above.\n\ndef gaussian(x_array, ampl, centre, width):\n    \"\"\"Generate a signal with a Gaussian shape.\"\"\"\n    sigma = width/np.sqrt(8*np.log(2))\n    return ampl*(1/(sigma*np.sqrt(2*np.pi)))*np.exp(-(x_array-centre)**2/(2*sigma**2))\n\ndef lorentzian(x_array, ampl, centre, width):\n    \"\"\"Generate a signal with a Lorentzian shape.\"\"\"\n    h_width = width/2\n    return ampl/np.pi * h_width/((x_array-centre)**2 + h_width**2)\n\n\n\nBefore using the functions to try fitting the NMR peaks, we can look at an example of what the two peak shapes look like by simulating a peak generated by the two functions and plotting them:\n\n# Write some code here to generate a peak of each type centred at 1, with a width of 0.1 and amplitude of 10.\n# Tip: the numpy linspace method makes it straightforward to generate an equally-spaced array of numbers for the\n# x-axis data.\n\n# Your code here...\n\n\n# Once you have the two peaks, plot them on the same set of axes to compare them.\n\n# Your code here...\n\n# Uncomment the line below and run the cell to see some code that works\n# %load ../code_snips/gen_peaks.txt\n\n\n\n\nWe now know that the functions can model the shape of Lorentzian and Gaussian peaks. So we should be able to use them to try to fit the peaks in the NMR spectrum. The fitting process optimises the model’s parameters to obtain a calculated spectrum that is as close as possible to the experimentally observed data.\n\nBoth the Lorentzian and Gaussian models have three parameters that can be varied to modify the peak shape: the amplitude, position of the peak’s centre and the peak’s width. Depending on the type of measurement, all three can provide information about the analyte.\n\nWe will use \n\nscipy.optimize.curve_fit to fit the peaks in the spectrum. It uses \n\nnon-linear least squares to fit a function - in this case the Lorentzian we defined - to a set of data.\n\nSee Also\n\nHere is a basic intro to \n\nLeast Squares Optimisation.\n\n","type":"content","url":"/chem-data/spectrum-analysis#peak-processing-in-scipy","position":7},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Peak finding - getting initial model parameters","lvl2":"Peak processing in scipy"},"type":"lvl3","url":"/chem-data/spectrum-analysis#peak-finding-getting-initial-model-parameters","position":8},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Peak finding - getting initial model parameters","lvl2":"Peak processing in scipy"},"content":"You might remember that the optimisation needs a set of initial guesses for the parameters to fit the curve. For one or two peaks, that is easily done by hand, but we can use scipy’s signal processing \n\nscipy.signal.find_peaks to do this in Python.\n\nThe find_peaks function locates local maxima in the 1D array it is passed by comparing data points with neighbouring values.\n\nTry running the function on the intensity data of the NMR spectrum.\n\n# Pass the NMR intensities to the find_peaks function and check the results (take a look at the `find_peaks` docs to see what it returns). \n# Note the shape of the array storing the indices of the peaks in the intensity array.\n\npeak_idx, peak_info = find_peaks(nmr_spec[\"intensity\"])\npeak_idx\n\n\n\narray([   1,    3,    5, ..., 6989, 6994, 6998], shape=(2320,))\n\nRunning the peak finding on the intensities without providing any extra constraints results in find_peaks locating over 2000 peaks in the spectrum, rather than the two we see when we plot the spectrum. The additional “peaks” are weak intensity local maxima. The vast majority (all but two) are points that are higher intensity than their surroundings, but not significantly higher than the backgground noise.\n\nfind_peaks can also take other arguments that filter the peaks it finds based on the properties of the peaks. The information it returns will also depend on the arguments passed.\nCheck the \n\ndocs and modify the call to find_peaks to filter the peaks and get the information required as initial guesses for the Lorentzian peaks.\n\n# Modify the call to `find_peaks` below to filter the peaks to just the two real resonances.\n# Your call should include parameters so that the function returns enough peak information to get initial values for \n# peak amplitude, centre and width.\n\npeak_idx, peak_info = find_peaks(nmr_spec[\"intensity\"])\n\n# uncomment the line below and run the cell to load the complete code\n# %load ../code_snips/find_peaks_filter.txt\n\n# Your code here...\n\n\nprint(peak_idx)\npeak_info\n\n\n\nfind_peaks is not aware of the data along the x-axis, i.e. the chemical shift, there is still some work to pull out the peak centres and the width of the peaks in ppm - at the moment, the widths are given in terms of the number of data points.\n\n# Use the indices of the peaks to work out the peak centres in ppm and add them to the peak_info dictionary with the key \"centres\".\n\n# Your code here...\n\n\n# uncomment the line below and run the cell to load some code to do this\n# %load ../code_snips/peak_centres_ppm.txt\n\n\n\n\n# The widths are currently expressed in terms of number of data points, i.e. \" 'widths': array([2.7734915 , 3.57548198]) \" \n# means the FWHM of the first peak is the distance between 2.77 points, which assumes they are evenly spaced.\n# Work out the widths in ppm. Update the widths in the peak_info dictionary to these values.\n# Hint: You can calculate the distance between adjacent rows in a pandas Series using the diff() method.\n\n\n# Your code here...\n\n\n# uncomment the line below and run the cell to load some code to do this\n# %load ../code_snips/widths_ppm.txt\n\n\n\n\n# Check the peak information\npeak_info\n\n\n\n# Tidy up the peak information\n\npeak_info = {key: val for key, val in peak_info.items() if key in [\"peak_heights\", \"widths\", \"centres\"]}\n\n\n\nOK! Finally, let’s try fitting the Lorentzian model to the NMR peaks.\n\n","type":"content","url":"/chem-data/spectrum-analysis#peak-finding-getting-initial-model-parameters","position":9},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Peak fitting - optimising the model to get precise peak information","lvl2":"Peak processing in scipy"},"type":"lvl3","url":"/chem-data/spectrum-analysis#peak-fitting-optimising-the-model-to-get-precise-peak-information","position":10},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Peak fitting - optimising the model to get precise peak information","lvl2":"Peak processing in scipy"},"content":"Now you are ready to run the least squares optimisation to fit the curves to the peaks in the spectrum.\n\nFor this simple spectrum, the peaks are well separated so can be fitted separately and the baseline is very low and flat. Check the \n\ncurve_fit docs if you need a reminder of how to call the function, passing in the information from the peak_info dictionary as the initial values for the parameters.\n\nOne thing to note: The amplitude of the fitted curve will be the integrated intensity - the area under the peak. You can estimate the initial area as that of a rectangle of height = peak height and width = peak width.\n\n# Run `curve_fit` for each of the peaks in the NMR spectrum and store the output in the lists popt_list and pcov_list\n\npopt_list = []\npcov_list = []\n\n# Your code here...\n\n\n\n\n# uncomment the line below and run the cell to load some code to do this\n# %load ../code_snips/run_curve_fits.txt\n\n\ndisplay(popt_list, pcov_list)\n\n\n\n\n","type":"content","url":"/chem-data/spectrum-analysis#peak-fitting-optimising-the-model-to-get-precise-peak-information","position":11},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Assessing the fit","lvl2":"Peak processing in scipy"},"type":"lvl3","url":"/chem-data/spectrum-analysis#assessing-the-fit","position":12},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Assessing the fit","lvl2":"Peak processing in scipy"},"content":"Check the optimised parameters of the fitted Lorentzian peaks stored in popt_list. These will be [amplitude, centre, width] for each peak.\n\nTake another look at the spectrum. Do these values look reasonable? It might be difficult to tell with the areas, but\nremember the rectangular approximation.\n\nThe covariance matrix for the fitted functions are in pcov_list. The diagonal elements are the variances of the parameters and these can be used to estimate the errors (uncertainties) on the parameters (see \n\ndocs for info).\n\n# Calculate the 1-standard deviation errors for each of the parameters of the peaks. \n# Report the parameter values and the associated errors.\n\n\ndef report_peak_fit(popt, perr):\n    \"\"\"Print peak function parameters and 1-sigma errors\"\"\"\n    report = (f\"Amplitude: {popt[0]:.3f} +/- {perr[0]:.3f}\\n\" \n              f\"Centre: {popt[1]:.4f} +/- {perr[1]:.4f}\\n\"\n              f\"Width: {popt[2]:.4f} +/- {perr[2]:.4f}\\n\")\n\n    print(report)\n\n# Write a comment to explain what this code is doing.\nerrors = [np.sqrt(np.diag(pcov)) for pcov in pcov_list]\n\nfor i, pk in enumerate(popt_list):\n    print(f\"Peak {i+1}\")\n    report_peak_fit(pk, errors[i])\n\n\n\n\nWe can also overlay a spectrum calculated from the optimised peak parameters to see if the fitted peaks look reasonable by eye.\n\n# This function simulates a spectrum using the specified peak shape function and a set of parameters passed as a dictionary.\nfrom simulate import simulate_spectrum\n\n\n\ndef collate_fitted_peak_parameters(popt):\n    \"\"\" Make a dictionary of peak parameters \"\"\"\n\n    parameters = [\"ampl\", \"centre\", \"width\"]\n    return dict(zip(parameters, popt))\n\nfitted_peaks = [collate_fitted_peak_parameters(popt) for popt in popt_list] # assemble a list of dictionaries for peak parameters\n\nlor_spec_x, lor_spec_y = simulate_spectrum(lorentzian, nmr_spec[\"ppm\"], fitted_peaks)\n\n\n\n# y_sigma = nmr_spec[\"intensity\"].std()\n# y_errors = [y_sigma for y in nmr_spec[\"intensity\"]]\n\nnmr_spec.plot(x=\"ppm\", \n              y=\"intensity\",\n              xlabel=\"Chemical shift $\\\\delta$ / ppm\",\n              ylabel=\"Intensity\",\n              kind=\"scatter\",\n              marker=\".\",\n              label=\"Measured\")\n\nplt.plot(lor_spec_x, lor_spec_y, color=\"red\", label=\"Calculated\")\n\nplt.legend()\nplt.xlim((70, 0))\nplt.title(r\"$\\mathregular{^{13}C}$ NMR spectrum of ethanol\")\nplt.show()\n\n\n\nDoes this look like a good fit? Try changing the x-limits to check the fit of the peaks more closely.\n\n","type":"content","url":"/chem-data/spectrum-analysis#assessing-the-fit","position":13},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Performance metrics","lvl2":"Peak processing in scipy"},"type":"lvl3","url":"/chem-data/spectrum-analysis#performance-metrics","position":14},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Performance metrics","lvl2":"Peak processing in scipy"},"content":"We can also use a variety of metrics to assess how well the modelled spectrum fits the experimental data.\n\nR2 (coefficient of determination) is a statistical measure of how well the model explains the variability of the dependent variable (here, the intensity). Its form is quite intuitive and it is defined as follows:R^2 = 1 - \\frac{SSR}{SST}\n\nwhere:\n\nSSR (Sum of Squared Residuals): also called the residual sum of squares (RSS), is the quantity minimised by least square. It measures the total squared differences between the observed values and the predicted values from the model.SSR = \\sum (y_i - \\hat{y}_i)^2\n\nSST (Total Sum of Squares): measures the total variability in the observed data (without any model), based on the mean \\bar{y}:SST = \\sum (y_i - \\bar{y})^2\n\nIf the model fits perfectly,  SSR = 0  and  R^2 = 1 , meaning all variability in the data is explained by the model.\n\nIf the model is no better than simply using the mean  \\bar{y} , then  SSR = SST  and  R^2 = 0 .\n\nIf the model is worse than using the mean (e.g., a bad fit),  R^2  can be negative.\n\nIn least squares fitting, reducing SSR improves the model fit and increases  R^2 , so a good fit has a high  R^2  close to 1.\n\nWe could calculate R2 manually (code in cell below), but scikit-learn’s \n\nmetrics package makes it straightforward to calculate many measures of model performance.\n\n# To calculate r^2:\n\n# residuals = lor_spec_y - nmr_spec[\"intensity\"]\n# squared_residuals = residuals ** 2\n# SSR = squared_residuals.sum()\n# SST = ((nmr_spec[\"intensity\"] - nmr_spec[\"intensity\"].mean())**2).sum()\n# r2 = 1-(SSR/SST)\n# print(r2)\n\n\n\n\n# Import the relevant function from sklearn.metrics to calculate r^2, the coefficient of determination\n\n\n# uncomment the line below and run the cell to load some code to do this\n# %load ../code_snips/r2_sklearn.txt\n\n\n\n\n","type":"content","url":"/chem-data/spectrum-analysis#performance-metrics","position":15},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl2":"Key Points Summary:"},"type":"lvl2","url":"/chem-data/spectrum-analysis#key-points-summary","position":16},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl2":"Key Points Summary:"},"content":"Real-World Data Modeling: In many areas of chemistry (e.g., NMR, IR spectroscopy), fitting models to experimental data is crucial for extracting meaningful information, such as peak positions and intensities.\n\nPeak Fitting: Fitting peaks with a model function helps extract more precise information about the data (e.g., determining peak centers, amplitudes, and widths) compared to raw data points.\n\nUse of Least Squares: Least squares optimisation plays a central role in fitting models to experimental data. It helps minimise the difference between the model predictions and the observed data, providing the best-fit parameters for the model.\n\nPractical Considerations: When fitting models, especially in noisy or complex data, it is important to consider the quality of the fit and how well the model reflects the underlying data. This exercise highlights common issues in fitting, such as choosing an appropriate model and evaluating the fit.\n\nIterative Process: Fitting is often an iterative process. You may need to try different initial guesses for the model parameters, refine the model, and evaluate how well the fit matches the data.\n\n","type":"content","url":"/chem-data/spectrum-analysis#key-points-summary","position":17},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl2":"Things to try and/or consider:"},"type":"lvl2","url":"/chem-data/spectrum-analysis#things-to-try-and-or-consider","position":18},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl2":"Things to try and/or consider:"},"content":"Write some code to add a line that shows the residuals as a difference plot below the final graph of the experimental and calculated data.\n\nRepeat the fit using the Gaussian function. Is this a better or worse model for the NMR peak shapes?\n\nWhat happens if the initial guesses for the peak function’s parameters are not close to the actual values? Try fitting the peaks with one of the centres far from the real location. What happens?\n\nWe treated the spectrum using a very generic peak fitting process. Specialised NMR analysis libraries have methods to deal with more complex data much more efficiently (but many will still be using least squares underneath). Can you think of what additional complexities 1H NMR might pose, for example? What issues might arise if peaks are much closer together?\n\nThe 7000 points of the NMR data have effectively been reduced to six numbers. This poses some questions about how data is stored, reported and used for further analysis. What factors might be important when making those decisions? Are there any disadvantages of only having the peak information available? Would the choice be different (how, why) in different scenarios?\n\nYou can see how it might be possible to automate this process for NMR spectra and other types of measured datasets. What steps would be needed now to interpret the information to translate it to knowledge about the molecule? How straightforward is this to automate?","type":"content","url":"/chem-data/spectrum-analysis#things-to-try-and-or-consider","position":19},{"hierarchy":{"lvl1":"Exploratory Data Analysis (EDA)"},"type":"lvl1","url":"/data-fundamentals/eda","position":0},{"hierarchy":{"lvl1":"Exploratory Data Analysis (EDA)"},"content":"","type":"content","url":"/data-fundamentals/eda","position":1},{"hierarchy":{"lvl1":"Exploratory Data Analysis (EDA)","lvl2":"What is EDA?"},"type":"lvl2","url":"/data-fundamentals/eda#what-is-eda","position":2},{"hierarchy":{"lvl1":"Exploratory Data Analysis (EDA)","lvl2":"What is EDA?"},"content":"Process of examining, summarising and visualising data\n\nA first “triaging” step in a workflow to assess a set of data.\n\nAims to uncover patterns, detect anomalies, and gain insights before applying formal models or statistical tests.","type":"content","url":"/data-fundamentals/eda#what-is-eda","position":3},{"hierarchy":{"lvl1":"Exploratory Data Analysis (EDA)","lvl2":"Why is EDA important?"},"type":"lvl2","url":"/data-fundamentals/eda#why-is-eda-important","position":4},{"hierarchy":{"lvl1":"Exploratory Data Analysis (EDA)","lvl2":"Why is EDA important?"},"content":"Understand data structure – Identify key variables, distributions, and relationships.\n\nDetect errors and biases – Spot missing values, outliers, or inconsistencies.\n\nGenerate hypotheses – Reveal trends that can guide further investigation.\n\nChoose appropriate models – Decide whether data fits assumptions for statistical methods or machine learning models.","type":"content","url":"/data-fundamentals/eda#why-is-eda-important","position":5},{"hierarchy":{"lvl1":"Exploratory Data Analysis (EDA)","lvl3":"Examples of EDA (chemistry context)","lvl2":"Why is EDA important?"},"type":"lvl3","url":"/data-fundamentals/eda#examples-of-eda-chemistry-context","position":6},{"hierarchy":{"lvl1":"Exploratory Data Analysis (EDA)","lvl3":"Examples of EDA (chemistry context)","lvl2":"Why is EDA important?"},"content":"Reaction kinetics – Plotting rate vs. concentration may reveal non-linear trends that indicate appropriate rate-order models.\n\nSpectroscopy – Visualising raw IR or NMR spectra can help identify baseline shifts requiring additional processing or unexpected peaks before analysis.\n\nMaterials science – Initial exploration of crystal structure datasets may reveal possible correlations between lattice parameters and properties.","type":"content","url":"/data-fundamentals/eda#examples-of-eda-chemistry-context","position":7},{"hierarchy":{"lvl1":"Exploratory Data Analysis (EDA)","lvl2":"EDA Techniques"},"type":"lvl2","url":"/data-fundamentals/eda#eda-techniques","position":8},{"hierarchy":{"lvl1":"Exploratory Data Analysis (EDA)","lvl2":"EDA Techniques"},"content":"The aim of EDA is an initial assessment of\n\nWhether values are “reasonable” or as expected.\n\nShape of data and distributions.\n\nRelationships between variables (features).\n\nTo achieve this, generally a variety of techniques are required; most commonly involving some/all of the following:\n\nDescriptive statistics\n\nMean, median, standard deviation\n\nQuantitative ways to assess and compare central tendency \u000b\t(e.g. mean/median/mode)\nSpread of data (e.g. range, standard deviation, percentile ranges)\n\nVisualisations\n\nHistograms, scatter plots, box plots\n\nMissing data checks\n\nHeatmaps, bar charts of missing values\n\nCorrelation analysis\n\nPairwise plots, correlation matrices\n\nQuantify strength and direction of relationships between variablesIf we can quantify the data distribution and feature correlation, why bother visualising the data?\n\nWhy is visualisation important - \n\nnotebook\n\nSee Also\n\nPlotting with Seaborn","type":"content","url":"/data-fundamentals/eda#eda-techniques","position":9},{"hierarchy":{"lvl1":"Notebook - Visualisation in EDA"},"type":"lvl1","url":"/data-fundamentals/eda-visualisation","position":0},{"hierarchy":{"lvl1":"Notebook - Visualisation in EDA"},"content":"\n\n","type":"content","url":"/data-fundamentals/eda-visualisation","position":1},{"hierarchy":{"lvl1":"Notebook - Visualisation in EDA","lvl2":"Visualisation"},"type":"lvl2","url":"/data-fundamentals/eda-visualisation#visualisation","position":2},{"hierarchy":{"lvl1":"Notebook - Visualisation in EDA","lvl2":"Visualisation"},"content":"{attribution=“John Tukey”}\n\nThe simple graph has brought more information to the data analyst’s mind than any other device.\n\n# Import statements\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import r2_score\n\n\n\n# set mpl fonts\nplt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'Lucida Grande', 'Verdana']\n\n\n\n# Helper functions for plotting\ndef get_stats(y: pd.Series) -> tuple[float, float]:\n#    stats = (np.mean(y), np.std(y), np.corrcoef(x, y)[0][1])\n    stats = (y.mean(), y.std())\n    return stats\n\ndef add_stats(ax, x, y):\n    mu, sigma = get_stats(y)\n    r = np.corrcoef(x, y)[0][1]\n    stats_text = (f'Mean y $\\\\mu$ = {mu:.2f}\\n'\n                  f'Stdev. y $\\\\sigma$ = {sigma:.2f}\\n'\n                  f'Correlation $r$ = {r:.2f}')\n    ax.text(0.95, 0.05, stats_text, fontsize=9, \n            transform=ax.transAxes, horizontalalignment='right')\n    \ndef get_linear_fit(x, y):\n    model = np.polyfit(x, y, deg=1) # y = mx + c (m = gradient, c = intercept)\n    m, c = model\n    predict = np.poly1d(model)\n    r2 = r2_score(y, predict(x))\n    return m, c, r2\n\ndef add_fit(ax, x, y):\n    m, c, r2 = get_linear_fit(x, y)\n    ax.axline(xy1=(0, c), slope=m, color=\"red\", linewidth=1) # xy1 provides defined point for line to pass through\n    fit_text = (f\"Fit\\nGradient: {m:.2f}\\nIntercept: {c:.2f}\\n\"\n                f\"y = {m:.2f}x + {c:.2f}\\n\"\n                f\"$r^2$ = {r2:.2f}\")\n    ax.text(0.05, 0.95, fit_text, fontsize=9, \n            transform=ax.transAxes, horizontalalignment='left', verticalalignment=\"top\")\n\n\n\n","type":"content","url":"/data-fundamentals/eda-visualisation#visualisation","position":3},{"hierarchy":{"lvl1":"Notebook - Visualisation in EDA","lvl2":"Anscombe’s quartet"},"type":"lvl2","url":"/data-fundamentals/eda-visualisation#anscombes-quartet","position":4},{"hierarchy":{"lvl1":"Notebook - Visualisation in EDA","lvl2":"Anscombe’s quartet"},"content":"Anscombe’s quartet (please don’t click the link until after you have worked through the notebook) consists of four simple datasets, each consisting of 11 pairs of x and y values.\n\nBased on the statistical measures of mean, standard deviation and the correlation coefficient between the two variables, the datasets look close to identical. The data highlights the importance of visualisation in EDA and going beyond looking at the simple summary statistics.\n\n# The csv contains pairs of (xy) data, so (x1, y1) form the first set of data, (x2, y2) is the second, etc.\n\nanscombe_df = pd.read_csv(\"data/anscombe_quartet.csv\")\nanscombe_df\n\n\n\nWe can compare the some simple summary statistics for the x and y columns of each dataset.\n\nThe mean and standard deviation give a measure of the centre and its spread, so we can look at those. \n\nPearson’s correlation coefficient, r measures the linear correlation between the x and y variables.\n\ny_cols = [\"y1\", \"y2\", \"y3\", \"y4\"]\nx_cols = [\"x1\", \"x2\", \"x3\", \"x4\"]\n\nprint(\"Summary statistics for x data:\")\ndisplay(anscombe_df[x_cols].describe().loc[[\"count\",\"mean\", \"std\"]])\n\nprint(\"Summary statistics for y data:\")\ndisplay(anscombe_df[y_cols].describe().loc[[\"count\",\"mean\", \"std\"]])\n\n\nfor idx, dataset in enumerate(zip(x_cols, y_cols)):\n    print(f\"Dataset {idx+1}\")\n    pearson_r = np.corrcoef(anscombe_df[dataset[0]], anscombe_df[dataset[1]])[0][1]\n    print(f\"Pearson correlation coefficient: {pearson_r:.3f}\\n\")\n\n\n\n\n\n\n\n\n\n\n\nThe summary statistics for each of the x columns and y columns are the same to two decimal places (the same precision as the data itself). The correlation coefficients are also the same.\n\nSo from these measures, the four datasets look very similar. To check this, we can visualise the datasets as a set of scatter plots.\n\n# Plotting function\ndef anscombe_plots(ans_df=anscombe_df, show_stats=False, show_fit=False):\n    \"\"\"Draw a 4x4 grid showing plots of the Anscombe quartet data\"\"\"\n    \n    INCLUDE_STATS = show_stats\n    INCLUDE_FIT = show_fit\n\n    fig, axs = plt.subplots(2,2, sharex=True, sharey=True, figsize=(8, 8))\n\n    for i, ax in enumerate(axs.flat):\n        x = ans_df.iloc[:, 2*i]\n        y = ans_df.iloc[:, 2*i+1]\n        ax.scatter(x, y)\n        ax.set_xlabel(\"x\")\n        ax.set_ylabel(\"y\")\n        data_label = f\"Data {i+1}\"\n        ax.text(0.95, 0.3, data_label, fontsize=16, transform=ax.transAxes, horizontalalignment=\"right\", verticalalignment='top')\n\n        if INCLUDE_STATS:\n            add_stats(ax, x, y)\n        \n        if INCLUDE_FIT:\n            add_fit(ax, x, y)\n\n\n    plt.xlim(0, 20)\n    plt.ylim(0, 14)\n    plt.xticks([5, 10, 15, 20])\n    plt.tight_layout()\n    plt.show()    \n\n\n\nanscombe_plots(anscombe_df, show_stats=True, show_fit=False)\n\n\n\nFrom the scatter plots we can immediately see that the datasets are quite distinct.\nIf we calculate a linear regression line through the data, do the coefficients of the equation help to distinguish the data numerically?\n\nanscombe_plots(anscombe_df, show_stats=True, show_fit=True)\n\n\n\nThe best fit line is also identical and the r-squared value indicates that numerically, at least, it fits all of the datasets equally well (or badly).\n\nHowever, we can see from the visualisations that the regression line is only really appropriate for the first dataset.\n\nIf we look at some of the other descriptive statistics, particularly relating to the data distribution (max/min, percentiles), these show there are differences between the data. But the simple scatter plot visualisations make the contrasts unmissable.\n\nIn the case of Anscombe’s quartet, the scatter plots are sufficient to highlight the differences. For more complex data, there are other visualisations that highlight the differences between the distributions of the datasets.\n\n","type":"content","url":"/data-fundamentals/eda-visualisation#anscombes-quartet","position":5},{"hierarchy":{"lvl1":"Notebook - Visualisation in EDA","lvl3":"Key points summary","lvl2":"Anscombe’s quartet"},"type":"lvl3","url":"/data-fundamentals/eda-visualisation#key-points-summary","position":6},{"hierarchy":{"lvl1":"Notebook - Visualisation in EDA","lvl3":"Key points summary","lvl2":"Anscombe’s quartet"},"content":"Visualisation is an essential part of exploratory data analysis.\n\nAlways explore your datasets visually before applying models or drawing conclusions.\n\n{attribution=“John Tukey”}\n\n“The first step in analyzing data is to look at it.”\n\n","type":"content","url":"/data-fundamentals/eda-visualisation#key-points-summary","position":7},{"hierarchy":{"lvl1":"Notebook - Visualisation in EDA","lvl4":"Things to try/consider","lvl3":"Key points summary","lvl2":"Anscombe’s quartet"},"type":"lvl4","url":"/data-fundamentals/eda-visualisation#things-to-try-consider","position":8},{"hierarchy":{"lvl1":"Notebook - Visualisation in EDA","lvl4":"Things to try/consider","lvl3":"Key points summary","lvl2":"Anscombe’s quartet"},"content":"See Also\n\nAnother dataset, called the \n\nDatasaurus Dozen has been compiled to highlight how valuable visualisation is when assessing\ndata. Data is available \n\nhere\n\nIn the case of Anscombe’s quartet, the scatter plots are sufficient to highlight the differences.\n\nFor more complex data, there are other visualisations that highlight the differences between the distributions of the datasets. The \n\nseaborn visualisation library makes it particularly straightforward to look at distributions of multiple variables and correlations between variables.\n\nSee Also\n\nPlotting with Seaborn\n\nWhat kinds of plot might help to highlight the differences between the datasets or identify issues that might affect how the data is modelled, e.g. for dataset 3, there is a single isolated point; how does this affect the best-fit linear model; how might this be identified?","type":"content","url":"/data-fundamentals/eda-visualisation#things-to-try-consider","position":9},{"hierarchy":{"lvl1":"Notebook - ML for thermochemical data prediction"},"type":"lvl1","url":"/ml-intro/ml-demo","position":0},{"hierarchy":{"lvl1":"Notebook - ML for thermochemical data prediction"},"content":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\nfrom rdkit import Chem\nfrom rdkit.Chem import (\n                        PandasTools,\n                        Descriptors\n                        )\n\nfrom rdkit.Chem.Draw import IPythonConsole\n\n\n\n","type":"content","url":"/ml-intro/ml-demo","position":1},{"hierarchy":{"lvl1":"Notebook - ML for thermochemical data prediction"},"type":"lvl1","url":"/ml-intro/ml-demo#notebook-ml-for-thermochemical-data-prediction","position":2},{"hierarchy":{"lvl1":"Notebook - ML for thermochemical data prediction"},"content":"\n\nWe can use the data you worked with for the EDA workshop to demonstrate how fit a simple ML model to predict the boiling point of organic compounds from a small set of descriptors.\n\nThis is an example of\n\nSupervised learning as applied to a regression task.\n\nSupervised learning\n\nThe model learns from data for which a label (known value or category) is associated with each input (set of feature values)\n\nRegression task\n\nModel predicts a continuous numerical value based on the input features\n\nbp_df = pd.read_csv(\"data/alcohol_acid_phys_data_cleaned.csv\")\nbp_df\n\n\n\nThe data is read in from a csv file of the data cleaned in the \n\nEDA workshop, so should be prepared, but we can quickly check.\n\n# Check for missing values\nbp_df.isna().sum()\n\n\n\n# Check data types\nbp_df.info()\n\n\n\nThe numerical types are as expected. The category type for the class column has not been automatically recognised, understandably. For now we will not be working with that column, so will leave as is.\n\nWe will start by dropping any rows without a SMILES string and also the melting point column as we are going to try to predict the boiling point.\n\n# Drop rows with missing SMILES string\nbp_df = bp_df[bp_df[\"SMILES\"] != \"not found\"].reset_index(drop=True)\n\nbp_df = bp_df.drop(columns=[\"mp / dC\"])\nbp_df\n\n\n\n# Add RDKit molecules to the dataframe\nPandasTools.AddMoleculeColumnToFrame(bp_df, smilesCol=\"SMILES\")\nbp_df\n\n\n\n# Adapted from https://greglandrum.github.io/rdkit-blog/posts/2022-12-23-descriptor-tutorial.html\n\ndef getMolDescriptors(mol, descriptor_list=None, missingVal=None):\n    ''' calculate the full list of descriptors for a molecule\n    \n        missingVal is used if the descriptor cannot be calculated\n    '''\n    res = {}\n    if not(descriptor_list):\n        descriptors = Descriptors._descList\n    # TODO: Add else clause to handle a list numbers corresponding to the descriptor indices\n    else:\n        descriptors = [Descriptors._descList[idx] for idx in descriptor_list]\n\n    for nm,fn in descriptors:\n        # some of the descriptor fucntions can throw errors if they fail, catch those here:\n        try:\n            val = fn(mol)\n        except:\n            # print the error message:\n            import traceback\n            traceback.print_exc()\n            # and set the descriptor value to whatever missingVal is\n            val = missingVal\n        res[nm] = val\n    return res\n\n\n\n# These are the descriptors selected to calculate for the molecules.\n# 118 NumHAcceptors\n# 119 NumHDonors\n# 27 BalabanJ - a topological descriptor expressing molecular connectivity and branching\n# 28 BertzCT - a topological complexity index\n# 83 TPSA - total polar surface area\n\ndescriptor_list = [118, 119, 27, 28, 83]\n\n\ncalc_descriptors = [getMolDescriptors(mol, descriptor_list=descriptor_list) for mol in bp_df[\"ROMol\"]]\n\n# Create a dataframe from the calculated descriptors\ndescriptor_df = pd.DataFrame(calc_descriptors)\n\n# Add the descriptors to the dataframe as new columns\nbp_df = pd.concat([bp_df, descriptor_df], axis=1)\nbp_df\n\n\n\n# To remind us of the relationship between the variables, we can plot a pairplot and heatmap\n# Pairplot\nsns.pairplot(bp_df, hue=\"Class\", diag_kind=\"kde\",  palette=\"viridis\")\nplt.show()\n\n\n\ncorr = bp_df.drop(columns=[\"Class\", \"IUPAC name\", \"SMILES\", \"ROMol\"]).corr()\n\nsns.heatmap(corr, annot=True)\nplt.show()\n\n\n\n# corr_abs = bp_df.drop(columns=[\"Class\", \"IUPAC name\", \"SMILES\", \"ROMol\"]).corr().abs()\n# upper = corr_abs.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n# upper\n\n\n\nThis is all looking rather busy - it would be better to look at subsets of the features.\n\nWhat we can see is:\n\nThe feature most strongly correlated with the target variable is the molecular weight.\n\nMolecular weight is very strongly correlated with number of carbons and hydrogens\n\nMost of the other features are only moderately or weakly correlated to the target variable.\n\n# A closer look at the correlation between the boiling point and the molecular weight\nsns.regplot(data=bp_df, x=\"Molweight g/mol\", y=\"bp / dC\",  fit_reg=True,  ci=None)\nplt.show()\n\n\n\nFor the moment, we will drop the #C and #H columns. We will see that analysing the initial model can help tell us about the importance of the features.\n\nWe will also drop the non-numerical features for this task.\n\ncols_drop = [\"#C\", \"#H\", \"IUPAC name\", \"SMILES\", \"ROMol\", \"Class\"]\n\nprep_df = bp_df.drop(columns=cols_drop)\nprep_df\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n\n# By convention, the target variable is denoted by y and the features are denoted by X\n\ny_full = prep_df[\"bp / dC\"]\nX_full = prep_df.drop(columns=[\"bp / dC\"])\n\n\n\n# train_test_split shuffles the data by default and splits it into training and testing sets. The \n# proportion of the data that is used for testing is determined by the test_size parameter. Here, \n# we are using 80 % of the data for training and 20% for the test set. \n# The random_state parameter is used to set the seed for the random number generator so that the \n# results are reproducible.\n\nX_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42)\n\n\n\n# Check the size of the training and testing sets\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n\n\nScikit-learn makes many models available via a consistent interface.\n\nWe are going to use a linear regression model for this task.\n\nfrom sklearn import linear_model\n\n\n\n# Create a linear regression model\nreg = linear_model.LinearRegression()\n\n# Fit the model to the training data\nreg.fit(X_train, y_train)\n\n\n\n# Predict the boiling points of the test set\ny_pred = reg.predict(X_test)\n\n\n\n\nWe can plot the boiling points that the model predicted for the test set against the known true values to see how good a job the model makes of the predictions\n\n\nplt.scatter(y_test, y_pred)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '-', lw=1, color=\"red\")\nplt.xlabel(\"True Values [bp / dC]\")\nplt.ylabel(\"Predictions [bp / dC]\")\nplt.show()\n\n\n\n# The r^2 value is a measure of how well the model fits the data. It ranges from 0 to 1, \n# with 1 indicating a perfect fit.\nr2 = reg.score(X_test, y_test)\nr2\n\n\n\nIf we look back, we can see that the Pearson correlation coefficient for the relationship between molecular weight and boiling point was 0.84, so the model predicts more accurately than using the molecular weight alone.\n\nThe model coefficients tell us about the weighting of the features used by the fitted model.\n\nprint(X_full.columns)\nreg.coef_\n\n\n\nHowever, because the magnitude of the features’ values are on different scales, the coefficients also incorporate the different scales.\n\nA scaler can be used to transform the features to a consistent scale. Here’s we’ll use a \n\nMinMaxScaler to transform the features to have a scale between 0 and 1.\n\n# Split the scaled data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42)\n\n# create a min/max scaler\nscaler = MinMaxScaler()\n\n# fit the scaler to the training data\nscaler.fit(X_train)\n\n# transform the training and testing data separately\nscaled_X_train = scaler.transform(X_train)\nscaled_X_test = scaler.transform(X_test)\n\n\n\n# Create a linear regression model\nreg = linear_model.LinearRegression()\n\n# Fit the model to the training data\nreg.fit(scaled_X_train, y_train)\n\n# Predict the boiling points of the test set\ny_pred = reg.predict(scaled_X_test)\n\n\n\nplt.scatter(y_test, y_pred)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '-', lw=1, color=\"red\")\nplt.xlabel(\"True Values [bp / dC]\")\nplt.ylabel(\"Predictions [bp / dC]\")\nplt.show()\n\n\n\n# calculate the R^2 score\nr2 = reg.score(scaled_X_test, y_test)\nr2\n\n\n\nThe model’s predictions look the same as before, but we can now look at the coefficients.\n\nprint(X_full.columns)\nreg.coef_\n\n\n\nWe can now see that the coefficients which represent the weights of the features in the fitted model indicate that molecular weight - as expected - and density are contributing most strongly to the model","type":"content","url":"/ml-intro/ml-demo#notebook-ml-for-thermochemical-data-prediction","position":3},{"hierarchy":{"lvl1":"Introduction to Machine Learning"},"type":"lvl1","url":"/ml-intro/ml-intro","position":0},{"hierarchy":{"lvl1":"Introduction to Machine Learning"},"content":"","type":"content","url":"/ml-intro/ml-intro","position":1},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl2":"What is machine learning?"},"type":"lvl2","url":"/ml-intro/ml-intro#what-is-machine-learning","position":2},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl2":"What is machine learning?"},"content":"Machine learning\n\nA subset of AI that involves the development of computer algorithms that enable computers to learn from data without being explicitly programmed.\n\nThe basic aim is to develop a model that maps an input (which is the data) to an output (the variable or target to be predicted).\n\nIn many cases, the model is a mathematical function - or a collection of functions - and to obtain a model that works effectively to map the input to the output.","type":"content","url":"/ml-intro/ml-intro#what-is-machine-learning","position":3},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Some examples","lvl2":"What is machine learning?"},"type":"lvl3","url":"/ml-intro/ml-intro#some-examples","position":4},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Some examples","lvl2":"What is machine learning?"},"content":"Given a dog’s breed, height and age, predict their weight.\n\nGiven a compound’s SMILES, molecular weight and total polar surface area, predict its boiling point.\n\nFrom a customer’s balance, income and employment status, predict whether they will become overdrawn in the next month.\n\nBased on an ionic compound’s formula, the charge and radii of its ions, predict if it will form a rock salt structure.","type":"content","url":"/ml-intro/ml-intro#some-examples","position":5},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl2":"Types of machine learning"},"type":"lvl2","url":"/ml-intro/ml-intro#types-of-machine-learning","position":6},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl2":"Types of machine learning"},"content":"","type":"content","url":"/ml-intro/ml-intro#types-of-machine-learning","position":7},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Supervised learning","lvl2":"Types of machine learning"},"type":"lvl3","url":"/ml-intro/ml-intro#supervised-learning","position":8},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Supervised learning","lvl2":"Types of machine learning"},"content":"Trains a model on a labelled dataset.\n\nEach input (set of features) is associated with a known output (target variable).\n\nAims to learn a function that maps inputs to outputs so that the model can make accurate predictions on unseen data.\n\nTraining minimises a loss function, which measures how far the model’s predictions are from true values.\n\nWidely used for both\n\nregression tasks (predicting continuous values).\n\nclassification tasks (assigning categories).\n\nUseful when clear relationships exist between inputs and outputs","type":"content","url":"/ml-intro/ml-intro#supervised-learning","position":9},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Unsupervised learning","lvl2":"Types of machine learning"},"type":"lvl3","url":"/ml-intro/ml-intro#unsupervised-learning","position":10},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Unsupervised learning","lvl2":"Types of machine learning"},"content":"Involves learning from unlabelled data.\n\nAims to identify patterns or structures in the data without predefined categories or values.\n\nMost common applications are\n\nclustering (grouping similar data points)\n\ndimensionality reduction (simplifying data while retaining significant information)\n\nBecause known “true” values are not available, there is no explicit error to guide the model training.\n\nLearning maximises similarity within clusters (clustering) or preserving variance in the data (dimensionality reduction).\n\nUseful for exploratory analysis when relationships in the data are unknown.","type":"content","url":"/ml-intro/ml-intro#unsupervised-learning","position":11},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Reinforcement learning","lvl2":"Types of machine learning"},"type":"lvl3","url":"/ml-intro/ml-intro#reinforcement-learning","position":12},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Reinforcement learning","lvl2":"Types of machine learning"},"content":"Involves an agent interacts with an environment to achieve a goal.\n\nThe agent takes actions and receives feedback in the form of rewards or penalties.\n\nReinforcent learning learns through trial and error, improving its strategy over time.\n\nThe algorithm aims to maximise the cumulative reward; this may require balancing short-term and long-term gains.\n\nIt is applied in fields such as robotics, game playing (e.g. AlphaGo) and self-driving cars.\n\nUseful when an explicit mapping from inputs to outputs is not available, but optimal decision-making is needed.\n\n\n\nSupervised Learning\n\nUnsupervised Learning\n\nReinforcement Learning\n\nData type\n\nLabelled (input-output pairs)\n\nUnlabelled (no predefined outputs)\n\nNo predefined labels, learns from rewards\n\nGoal\n\nLearn a function to map inputs to known outputs\n\nDiscover hidden patterns or structures\n\nLearn best actions to maximise cumulative rewards\n\nExamples tasks\n\nRegression, classification\n\nClustering, dimensionality reduction\n\nDecision-making, control problems\n\nTraining process\n\nMinimises a loss function based on known outputs\n\nFinds structure without explicit output labels\n\nLearns via trial and error with rewards/penalties\n\nCommon algorithms\n\nLinear regression, decision trees, neural networks\n\nk-means clustering, PCA, DBSCAN\n\nQ-learning, deep Q-networks (DQN), policy gradient methods\n\nUse cases\n\nPredicting chemical properties, image recognition\n\nGrouping molecules by structure, anomaly detection\n\nRobotics, game AI, self-driving cars","type":"content","url":"/ml-intro/ml-intro#reinforcement-learning","position":13},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl2":"Machine learning process"},"type":"lvl2","url":"/ml-intro/ml-intro#machine-learning-process","position":14},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl2":"Machine learning process"},"content":"We can summarise a generalised machine learning process as having four main components:\n\nDataset\n\nModel\n\nTraining\n\n[Validation] - not always required, depending on model/context\n\nTesting","type":"content","url":"/ml-intro/ml-intro#machine-learning-process","position":15},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Data","lvl2":"Machine learning process"},"type":"lvl3","url":"/ml-intro/ml-intro#data","position":16},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Data","lvl2":"Machine learning process"},"content":"\n\nUsing a familiar example of tabular data, we are used to seeing the rows in the table as representing observations or samples.\n\nThe columns then represent variables, which as scientists, we often split into independent variables (those we control) and dependent or response variables (those we expect to vary in response).\n\nIn data vocabulary, the variables are often referred to as features and the target variable:\n\nNote\n\nFeatures, predictors (predictor variables)\n\nIndependent variables\n\nTarget\n\nDependent variable (or whatever you want to predict)\n\nAs we will see shortly, the model’s performance at predicting for data it has seen and new data is assessed separately. So the complete dataset needs to be split into separate parts so that the accuracy of the model’s predictions are evaluated fairly. \n\nSplitting the data into training and test sets.","type":"content","url":"/ml-intro/ml-intro#data","position":17},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Model","lvl2":"Machine learning process"},"type":"lvl3","url":"/ml-intro/ml-intro#model","position":18},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Model","lvl2":"Machine learning process"},"content":"\n\nA model maps the input data (features) to the target output\n\nThe model usually takes the form of a mathematical function\n\nTraining the model involves finding its best parameters to minimise loss\n\nThe internal model parameters are adjusted to minimise the difference between predictions and true values (if available)","type":"content","url":"/ml-intro/ml-intro#model","position":19},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Training","lvl2":"Machine learning process"},"type":"lvl3","url":"/ml-intro/ml-intro#training","position":20},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Training","lvl2":"Machine learning process"},"content":"Training involves adjusting the model’s parameters to fit the data.\n\nThe model is given a subset of the data called the training set.\n\nThe process adjusts the model’s parameters to minimise a loss function.\n\nLoss represents the difference between true values and prediction.\n\nMore complicated for unlabelled data (where true values are not available).\n\nAims to produce a model that generalises well to unseen data.\n\nAvoid overfitting – where the model performs well in training by ‘memorising’ training data, but will perform poorly against unseen data.\n\nAvoid underfitting – there the model does not capture the patterns in the data.","type":"content","url":"/ml-intro/ml-intro#training","position":21},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Validation","lvl2":"Machine learning process"},"type":"lvl3","url":"/ml-intro/ml-intro#validation","position":22},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Validation","lvl2":"Machine learning process"},"content":"Validation assesses how well the model generalises before final testing.\n\nThe model is given a subset of the data called the validation set (this is separate from training data).\n\nThe process guides model tuning, helping to optimise hyperparameters (these parameters control how the model learns, but are not learned from the data itself e.g., regularisation strength, learning rate).\n\nAims to find a model that performs well on unseen data by preventing overfitting.\n\nOverfitting detection – if validation error is much higher than training error, the model is too complex.\n\nUnderfitting detection – if both training and validation errors are high, the model may be too simple.\n\nSometimes uses cross-validation, where different subsets of data are used for validation in turns, particularly if the quantity of data is limited.\n\nNot used for final model evaluation – that is the role of testing.\n\nValidation is sometimes skipped, e.g. for simple models without hyperparameters to tune, models where hyperparameters are fixed, model is used for EDA or prototyping.","type":"content","url":"/ml-intro/ml-intro#validation","position":23},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Testing","lvl2":"Machine learning process"},"type":"lvl3","url":"/ml-intro/ml-intro#testing","position":24},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Testing","lvl2":"Machine learning process"},"content":"Testing evaluates the final model’s performance on previously unseen data.\n\nThe model is given a subset of the data called the test set (separate from training and validation data).\n\nThe aim is to assess generalisation, estimating how well the model will perform on real-world data it has not seen before.\n\nUnlike validation, no further adjustments of the model are made based on test results.\n\nKey performance metrics are calculated, such as:\n\nFor regression: Mean Squared Error (MSE), R²\n\nFor classification: Accuracy, Precision, Recall, F1-score\n\nThe test set should be used only once to ensure an unbiased evaluation.\n\nA good test score does not guarantee real-world success.\n\nNew data might show shifts in distribution or contain different patterns, which can still cause issues.","type":"content","url":"/ml-intro/ml-intro#testing","position":25},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl2":"Preparing a dataset"},"type":"lvl2","url":"/ml-intro/ml-intro#preparing-a-dataset","position":26},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl2":"Preparing a dataset"},"content":"","type":"content","url":"/ml-intro/ml-intro#preparing-a-dataset","position":27},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Feature selection","lvl2":"Preparing a dataset"},"type":"lvl3","url":"/ml-intro/ml-intro#feature-selection","position":28},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Feature selection","lvl2":"Preparing a dataset"},"content":"Feature selection aims to choose the most relevant input variables (features) to improve model performance.\n\nYou saw previously that EDA can be important in identifying which variables of a dataset are important and whether there are strong correlations between variables.\n\nThis is important when selecting which features to include in the dataset to develop a ML model.\n\nFeature selection can:\n\nreduce overfitting by removing irrelevant/noisy features.\n\nimprove model efficiency and reduce computation time.\n\nenhance interpretability, especially in scientific applications.\n\nFeatures can be selected on the basis of EDA or specific statistical tests (e.g. correlation, mutual information) to rank features. Alternatively, you can train models with different feature subsets and select the best-performing set.\n\nFor some models, e.g. lasso regression, decision trees, feature selection is built into the model.","type":"content","url":"/ml-intro/ml-intro#feature-selection","position":29},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Splitting the dataset","lvl2":"Preparing a dataset"},"type":"lvl3","url":"/ml-intro/ml-intro#splitting-the-dataset","position":30},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Splitting the dataset","lvl2":"Preparing a dataset"},"content":"As we saw in the overview, the trained model needs to generalise to make predictions for data it has not previously seem.\n\nSplitting the data into separate training and test sets helps assess whether the model generalises well and avoids misleading performance estimates.\n\nTraining Set\n\nThe model learns patterns in the training data by adjusting its parameters.\n\nIf the model is only assessed against the data it has trained on, the model might perform well by memorises the data (overfitting) rather than generalising.\n\nValidation Set\n\nUsed to tune hyperparameters and make adjustments without seeing the test set.\n\nHelps detect overfitting: If training performance is good but validation performance is poor, the model is memorising rather than learning.\n\nEnsures best version of the model is selected before final testing.\n\nTest Set\n\nUsing a separate test set provides a more realistic measure of how well the model will perform on unseen data.\n\nUsing the same data for training and testing, risks optimistic and misleading results.\n\nEnsures we the model is not “tweaked” based on test performance, avoiding hidden overfitting.","type":"content","url":"/ml-intro/ml-intro#splitting-the-dataset","position":31},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Train test split","lvl2":"Preparing a dataset"},"type":"lvl3","url":"/ml-intro/ml-intro#train-test-split","position":32},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Train test split","lvl2":"Preparing a dataset"},"content":"scikit-learn provides a convenient method \n\ntrain_test_split to ensure the dataset order is shuffled and then splits the full dataset into test and training set.\n\nIf a validation set is needed, the initial train set can be further split to form the final training and validation set.","type":"content","url":"/ml-intro/ml-intro#train-test-split","position":33},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl4":"Cross-validation","lvl3":"Train test split","lvl2":"Preparing a dataset"},"type":"lvl4","url":"/ml-intro/ml-intro#cross-validation","position":34},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl4":"Cross-validation","lvl3":"Train test split","lvl2":"Preparing a dataset"},"content":"Splitting the data into three separate sets for training, validation and testing limits the data available for training the model. It also means the outcome can be dependent on the specific (albeit random) split to form the training and validation sets.\n\nCross-validation (CV) is an alternative to using a fixed validation set. Instead of separating a single validation set, it rotates through different subsets of data to get a more reliable estimate of model performance and removes the need to remove a portion of the data from training.\n\nCommon cross-validation methods\n\nk-Fold Cross-Validation\n\nThe training data is split into k subsets (folds).\n\nThe model is trained on (k-1) folds and validated on the remaining fold.\n\nThis repeats k times, using a different validation fold each time.\n\nThe final model performance is the average across all folds.\n\nLeave-One-Out Cross-Validation (LOOCV):\n\nUses only one data point as validation in each iteration.\n\nMore computationally expensive but useful for very small datasets.\n\nMore details and info on many other methods are available in the \n\nsklearn user guide","type":"content","url":"/ml-intro/ml-intro#cross-validation","position":35},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl2":"Key challenges in developing an ML solution"},"type":"lvl2","url":"/ml-intro/ml-intro#key-challenges-in-developing-an-ml-solution","position":36},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl2":"Key challenges in developing an ML solution"},"content":"","type":"content","url":"/ml-intro/ml-intro#key-challenges-in-developing-an-ml-solution","position":37},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Overfitting vs. Underfitting (Bias-Variance tradeoff)","lvl2":"Key challenges in developing an ML solution"},"type":"lvl3","url":"/ml-intro/ml-intro#overfitting-vs-underfitting-bias-variance-tradeoff","position":38},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Overfitting vs. Underfitting (Bias-Variance tradeoff)","lvl2":"Key challenges in developing an ML solution"},"content":"Variance\n\nError due to overly complex models that are highly sensitive to training data. Leads to overfitting.\n\nOverfitting\n\nThe model learns patterns too specific to the training data, including noise, leading to poor generalisation to new data.\n\nBias\n\nError due to oversimplified models (e.g., assuming linearity where relationships are nonlinear). Leads to underfitting.\n\nUnderfitting\n\nThe model is too simple and fails to capture underlying patterns, resulting in poor performance on both training and test data.\n\nTradeoff\n\nA balance is needed—reducing bias increases variance, and vice versa. Cross-validation helps find the right complexity.\n\nPossible solutions\n\nRegularisation (e.g., L1/L2 penalties) for overfitting\n\nReduce (overfitting) /increase (underfitting) model complexity\n\nIncreasing training data (overfitting)\n\nCross-validation, e.g. to reduce overfitting of single training split and better estimate of generalisation.","type":"content","url":"/ml-intro/ml-intro#overfitting-vs-underfitting-bias-variance-tradeoff","position":39},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Data Quality & Feature Selection","lvl2":"Key challenges in developing an ML solution"},"type":"lvl3","url":"/ml-intro/ml-intro#data-quality-feature-selection","position":40},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Data Quality & Feature Selection","lvl2":"Key challenges in developing an ML solution"},"content":"Garbage In, Garbage Out\n\nPoor data (missing values, noise, irrelevant features) leads to unreliable models.\n\nFeature Selection\n\nChoosing the most informative features improves accuracy and reduces overfitting.\n\nDimensionality Reduction\n\nTechniques like PCA help manage high-dimensional data while retaining essential information.\n\nImbalanced Data\n\nIn classification tasks, skewed class distributions can bias the model toward majority classes.","type":"content","url":"/ml-intro/ml-intro#data-quality-feature-selection","position":41},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Model Evaluation","lvl2":"Key challenges in developing an ML solution"},"type":"lvl3","url":"/ml-intro/ml-intro#model-evaluation","position":42},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl3":"Model Evaluation","lvl2":"Key challenges in developing an ML solution"},"content":"Metrics vary by task\n\nRegression: RMSE, MAE, R²\n\nClassification: Accuracy, Precision, Recall, F1-score, Confusion Matrix\n\nOverreliance on Accuracy\n\nIn imbalanced datasets, accuracy alone can be misleading—alternative metrics like Precision-Recall or ROC-AUC should be considered.\n\nCross-validation\n\nensures robust performance estimation, preventing reliance on a single data split.","type":"content","url":"/ml-intro/ml-intro#model-evaluation","position":43},{"hierarchy":{"lvl1":"Chemical Data, Discovery and Design"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Chemical Data, Discovery and Design"},"content":"Canvas site\n\nThis book hopefully acts as the main resource for content for the first half of CHEM502.\n\nThis book was originally written by Sam Chong and is now maintained and updated by Joe Forth. Please let Joe (\n\nj​.forth@liverpool​.ac​.uk) know if you spot any errors, cannot access any of the expected content, have suggestions for additional content or resources to include.\n\nYou can access the GitHub repository containing the markdown files and notebooks \n\nhere. The code is available in the book directory and is broken down into topics.\n\nThere is a readme and environment yaml/requirements files in the root directory of the repository which should hopefully enable you create a conda environment in which you can run the notebooks.\n\nCHEM502 - Chemical Data, Discovery, and Design\n\nPart 2 - Chemical Data\n\nMolecular descriptors and similarity\n\nNotebook exercise - molecular fingerprints\n\nNotebook exercise - working with spectral data (or patterns)\n\nLeast squares optimisation\n\nPart 2 - Data Fundamentals\n\nExploratory Data Analysis (EDA)\n\nNotebook - Visualisation in EDA\n\nWorkshop - Exploratory Data Analysis\n\nPart 3 - Intro to Machine Learning\n\nIntroduction to Machine Learning\n\nNotebook - ML for thermochemical data prediction\n\nPart 4 - Data in the Digital Chemistry Lab\n\nStructured data and digital chemistry\n\nWarning\n\nThe book is under active development. Content will be added throughout the semester.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Workshop - Exploratory Data Analysis"},"type":"lvl1","url":"/workshop-files/eda-workshop","position":0},{"hierarchy":{"lvl1":"Workshop - Exploratory Data Analysis"},"content":"In this workshop, we will work with a dataset of thermochemical data for some molecules to explore what features or descriptors are influential in their melting and/or boiling points.\n\n","type":"content","url":"/workshop-files/eda-workshop","position":1},{"hierarchy":{"lvl1":"Workshop - Exploratory Data Analysis","lvl2":"Useful resources"},"type":"lvl2","url":"/workshop-files/eda-workshop#useful-resources","position":2},{"hierarchy":{"lvl1":"Workshop - Exploratory Data Analysis","lvl2":"Useful resources"},"content":"We will be using some of the python libraries you have already seen and Seaborn, which you might not have yet. Here are some quick start guides and/or tutorials that might come in useful.\n\nPandas\n\n10 minutes to pandas\n\nMatplotlib\n\nQuick start guide\n\nRDKit\n\nGetting started with the RDKit in Python\n\nRDKit tutorial from 2021 - this covers a lot of ground. We won’t be talking about reactions (towards end of notebook)\n\nThere are also lots of videos on YouTube and of course ChatGPT (though I am not sure how well it does with RDKit, probably because the documentation is patchy).\n\nYou might also find some useful bits and pieces in the \n\nMolecular fingerprints notebook in the module book.\n\nNote\n\nYou can find a notebook with code for the data cleaning, visualisation of the initial data and calculation of molecular descriptors \n\nhere.\n\n","type":"content","url":"/workshop-files/eda-workshop#useful-resources","position":3},{"hierarchy":{"lvl1":"Workshop - Exploratory Data Analysis","lvl2":"Visualising factors affecting thermochemical properties  of organic compounds"},"type":"lvl2","url":"/workshop-files/eda-workshop#visualising-factors-affecting-thermochemical-properties-of-organic-compounds","position":4},{"hierarchy":{"lvl1":"Workshop - Exploratory Data Analysis","lvl2":"Visualising factors affecting thermochemical properties  of organic compounds"},"content":"Let’s start by importing some libraries:\n\ntime (needed to include a sleep)\n\nrequests\n\npandas\n\nnumpy\n\nmatplotlib\n\nseaborn\n\n# TODO: Write your import statements here.\n\n\n# rdkit has a complicated structure, so we will start with these and maybe add some later\n\nfrom rdkit import Chem\nfrom rdkit.Chem import (\n                        AllChem,\n                        rdCoordGen,\n                        Draw,\n                        rdFingerprintGenerator,\n                        PandasTools,\n                        Descriptors\n                        )\n\nfrom rdkit.Chem.Draw import IPythonConsole\nfrom rdkit import DataStructs\n\nfrom IPython.display import SVG\nfrom ipywidgets import interact,fixed,IntSlider\n\n\n\n","type":"content","url":"/workshop-files/eda-workshop#visualising-factors-affecting-thermochemical-properties-of-organic-compounds","position":5},{"hierarchy":{"lvl1":"Workshop - Exploratory Data Analysis","lvl3":"Loading the data","lvl2":"Visualising factors affecting thermochemical properties  of organic compounds"},"type":"lvl3","url":"/workshop-files/eda-workshop#loading-the-data","position":6},{"hierarchy":{"lvl1":"Workshop - Exploratory Data Analysis","lvl3":"Loading the data","lvl2":"Visualising factors affecting thermochemical properties  of organic compounds"},"content":"The data is stored in a flat csv file in the data directory called alcohol_acid_phys_data.csv.\n\nCheck the data in the file (try the ‘head’ command)\n\nRead the data into a pandas dataframe\n\nDisplay the dataframe\n\n# TODO:\n\n# 0. Check the data in the file (try the 'head' command)\n# 1. Read the data into a pandas dataframe\n# 2. Display the dataframe\n\n\n\n\n\n","type":"content","url":"/workshop-files/eda-workshop#loading-the-data","position":7},{"hierarchy":{"lvl1":"Workshop - Exploratory Data Analysis","lvl3":"Cleaning the data","lvl2":"Visualising factors affecting thermochemical properties  of organic compounds"},"type":"lvl3","url":"/workshop-files/eda-workshop#cleaning-the-data","position":8},{"hierarchy":{"lvl1":"Workshop - Exploratory Data Analysis","lvl3":"Cleaning the data","lvl2":"Visualising factors affecting thermochemical properties  of organic compounds"},"content":"We need to do at least a little cleaning of the data. We can check the data for the number of rows and the data types in each column using \n\nDataFrame.info() method.\n\nThere are lots of pKa values missing. We are not going to use the pKa values, so we can drop those columns.\n\nSome rows are missing densities. And more importantly, some are missing melting and/or boiling points, which is the property we are interested in.\n\nIt might be possible to look these up somewhere, like the \n\nNIST Chemistry WebBook which unfortunately does seem not have a convenient API (there are unofficial ones if you search on the web). For now we can also drop these rows.\n\n# TODO:\n# 1. Drop the two pKa columns\n# 2. Drop the rows with NaN values in density, melting point and boiling point columns.\n# 3. Check the info again to see if the changes have been made.\n\n\n\n\n\nStill a few issues:\n\nThe Class and IUPAC name columns have some odd characters which appear to encode whitespace, e.g. Alkanedioic\\r\\nacid.\n\nThe .info() shows that the melting and boiling points have object, i.e. string data types, which suggests there are non-numerical values. If you look at the columns, some numbers have “d” or “s” sometimes with a number, probably to denote “decomposed” or “sublimed” maybe.\n\nPandas has str.contains and str.replace methods for its Series structure. Try using these to check and remove the encoded characters in those columns.\n\nCan you think of a way to deal with the non- or partly numeric phase change values?\n\nHint\n\nCould \n\nthis help?\n\n# TODO:\n\n# 1. Ensure only numeric values are present in the melting point, boiling point columns\n# 2. Remove the encoded whitespace characters from the 'Class' and 'IUPAC name' columns\n# 3. Convert the melting point, boiling point columns to numeric values.\n\n\n\nSome of the compounds do not have common names. We could either drop the column or fill the missing values with something like “unknown” or “none”.\n\n# TODO:\n\n# Clean column with missing compounds' common names\n\n\n\n\nIf you converted the mp and bp columns to numeric types using pd.to_numeric with errors=\"coerce\" then you will probably now have some additional null values in those columns, so those rows can be dropped.\n\n# TODO: Drop any remaining rows with NaN values in mp/bp columns\n\n\n\n\nFinally, we have a clean dataset with no missing values and the correct dtypes.\n\nWe can look at the summary statistics for the numerical columns we currently have, but there’s not much there yet.\n\nThere is one more thing we can do to tidy this data.\n\nYou may not be so familiar with the pandas category dtype. It is used when a variable takes a limited number of values.\n\nCheck the number of unique values for the columns. Which one could be treated as categorical data?\n\n# TODO: Check for categorical columns and change the data type to 'category' if necessary\n\n\n\n\n\n","type":"content","url":"/workshop-files/eda-workshop#cleaning-the-data","position":9},{"hierarchy":{"lvl1":"Workshop - Exploratory Data Analysis","lvl3":"Visualising the data","lvl2":"Visualising factors affecting thermochemical properties  of organic compounds"},"type":"lvl3","url":"/workshop-files/eda-workshop#visualising-the-data","position":10},{"hierarchy":{"lvl1":"Workshop - Exploratory Data Analysis","lvl3":"Visualising the data","lvl2":"Visualising factors affecting thermochemical properties  of organic compounds"},"content":"Have a look at this brilliant \n\nseaborn tutorial developed as by Charles J. Weiss at Augustana University in South Dakota.\n\nSome of the data used has a similar structure to this dataset.\n\nThere are no hard and fast rules about which types of plots to use to visualise your data, but the data types of the columns will mean some are more suitable to look at the data and relationships for certain variables.\n\nTry plotting the data to visualise some of the following:\n\nThe distribution of different classes of compound in the data set\n\nIdentify if there are any outliers for the thermochemical data or density\n\nThe distribution of boiling points, melting point and/or density with the class of the compound\n\nIdentify any correlations between the numerical features and the melting and/or boiling point.\n\nIs there any difference for different classes of compound?\n\nAre there any other interesting patterns or trends in the data that you have observed?\n\n","type":"content","url":"/workshop-files/eda-workshop#visualising-the-data","position":11},{"hierarchy":{"lvl1":"Workshop - Exploratory Data Analysis","lvl3":"Adding some descriptors","lvl2":"Visualising factors affecting thermochemical properties  of organic compounds"},"type":"lvl3","url":"/workshop-files/eda-workshop#adding-some-descriptors","position":12},{"hierarchy":{"lvl1":"Workshop - Exploratory Data Analysis","lvl3":"Adding some descriptors","lvl2":"Visualising factors affecting thermochemical properties  of organic compounds"},"content":"We have a list of compounds and a small number of observed values and descriptors. We can add a few more by calculating them using RDKit, but we only have IUPAC names, so we need to obtain a more rigorous representation to use with RDKit.\n\nThe \n\nChemical Identifier Resolver (CIR) service is run by the CADD Group at the NCI/NIH as part of their \n\nCactus server. It is used in the \n\nMolecular fingerprints notebook.\n\n# Here is a function so the process of getting the SMILES can be repeated for multiple compounds.\n# It includes a sleep time (`time.sleep`) to avoid overloading the server.\n\ndef get_smiles_from_name(name):\n    \"\"\"Gets SMILES string from the Cactus API given a chemical name.\"\"\"\n    \n    ROOT_URL = \"https://cactus.nci.nih.gov/chemical/structure/\"\n    identifier = name\n    representation = \"smiles\"\n\n    query_url = f\"{ROOT_URL}{identifier}/{representation}\"\n\n    response = requests.get(query_url)\n    time.sleep(0.05)\n    if response:\n        return response.text\n    else:\n        print(f\"Failed to get SMILES for {name}\")\n        return \"not found\"\n        # raise Exception(f\"Cactus request failed for {name}: {response.status_code}\")\n\n\n\n\n# TODO: Get a list of SMILES strings for the compounds in the dataframe and add this to the \n# dataframe as a new column.\n\n\n\n\n\nLet’s generate some descriptors for these molecules using RDKit.\n\nThere is a \n\ntutorial on calculating descriptors, and they are listed in the \n\nGetting Started guide.\n\nRDKit needs a RDKit.molecule to calculate the descriptors. You can create a separate list of molecules based on the SMILES strings in the dataframe, or you can use RDKit’s \n\nPandasTools module to work with them in a DataFrame.\n\nHave a look at the \n\nmolecular fingerprints notebook for some code to get started getting the RDKit molecules.\n\nChoose around 5 additional descriptors to calculate for each compound.\n\nIt is up to you how you handle the calculations and getting the new data combined with the existing dataframe.\n\nHere is one option:\n\nYou could use the getMolDescriptors function in the \n\ndescriptors tutorial as starting point to calculate the new descriptors and add them to dictionary that can be read into a dataframe.\n\nYou can then use \n\npd.concat to combine the dataframe with your thermochemical data with the new descriptors.\n\n# Add RDKit molecule objects to the dataframe\n\n\n\n\nfor idx, desc in enumerate(Descriptors.descList):\n    print(f\"{idx} {desc[0]}\")\n\n\n\n# From https://greglandrum.github.io/rdkit-blog/posts/2022-12-23-descriptor-tutorial.html\n\ndef getMolDescriptors(mol, descriptor_list=None, missingVal=None):\n    ''' calculate the full list of descriptors for a molecule\n    \n        missingVal is used if the descriptor cannot be calculated\n    '''\n    res = {}\n    if descriptor_list is None:\n        for nm,fn in Descriptors._descList:\n            # some of the descriptor fucntions can throw errors if they fail, catch those here:\n            try:\n                val = fn(mol)\n            except:\n                # print the error message:\n                import traceback\n                traceback.print_exc()\n                # and set the descriptor value to whatever missingVal is\n                val = missingVal\n            res[nm] = val\n    # TODO: Add else clause to handle a list numbers corresponding to the descriptor indices\n    else:\n        pass\n    return res\n\n\n\n# TODO: Add the descriptors to the dataframe as new columns\n\n\n\n\n\n","type":"content","url":"/workshop-files/eda-workshop#adding-some-descriptors","position":13},{"hierarchy":{"lvl1":"Workshop - Exploratory Data Analysis","lvl3":"Back to visualisation","lvl2":"Visualising factors affecting thermochemical properties  of organic compounds"},"type":"lvl3","url":"/workshop-files/eda-workshop#back-to-visualisation","position":14},{"hierarchy":{"lvl1":"Workshop - Exploratory Data Analysis","lvl3":"Back to visualisation","lvl2":"Visualising factors affecting thermochemical properties  of organic compounds"},"content":"Using your new seaborn skills, visualise the distributions and identify any correlations in your new data.\n\nYou will probably find plots like pairplots or heatmaps of more use now that you have a few more variables.\n\n\n\n","type":"content","url":"/workshop-files/eda-workshop#back-to-visualisation","position":15},{"hierarchy":{"lvl1":"Workshop - Exploratory Data Analysis","lvl3":"Summary","lvl2":"Visualising factors affecting thermochemical properties  of organic compounds"},"type":"lvl3","url":"/workshop-files/eda-workshop#summary","position":16},{"hierarchy":{"lvl1":"Workshop - Exploratory Data Analysis","lvl3":"Summary","lvl2":"Visualising factors affecting thermochemical properties  of organic compounds"},"content":"You have used the pandas library to clean and prepare a dataset, and to get descriptive statistics for the data.\n\nYou have visualised distributions and relationships in the data to look for anomalies and patterns.\n\nYou have used an API to obtain molecular identifiers/representations for a set of compounds.\n\nYou have generated molecular descriptors for a set of compounds using RDKit.","type":"content","url":"/workshop-files/eda-workshop#summary","position":17},{"hierarchy":{"lvl1":"Structured data and digital chemistry"},"type":"lvl1","url":"/workshop-files/lab-data-structure","position":0},{"hierarchy":{"lvl1":"Structured data and digital chemistry"},"content":"# If you do not have sqlite installed, you can install it by uncommenting the following command:\n#!conda install -y sqlite\n\n# If you do not have sqlalchemy installed, you can install it by uncommenting the following command:\n#!conda install -y sqlalchemy\n\n# only needed for mySQL\n#!conda install -y mysql-connector-python\n\n\n\nimport pandas as pd\n\nimport sqlalchemy as db\nfrom sqlalchemy import inspect\n# from sqlalchemy import create_engine\n\n\n\n","type":"content","url":"/workshop-files/lab-data-structure","position":1},{"hierarchy":{"lvl1":"Structured data and digital chemistry"},"type":"lvl1","url":"/workshop-files/lab-data-structure#structured-data-and-digital-chemistry","position":2},{"hierarchy":{"lvl1":"Structured data and digital chemistry"},"content":"","type":"content","url":"/workshop-files/lab-data-structure#structured-data-and-digital-chemistry","position":3},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl2":"Data storage in the automated lab"},"type":"lvl2","url":"/workshop-files/lab-data-structure#data-storage-in-the-automated-lab","position":4},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl2":"Data storage in the automated lab"},"content":"Being able to carry out data processing and analysis in python and then adding the transformed data to a database management system (DBMS) can not only makes the data flows in research labs more effective, but also aligns with FAIR principles.\n\nThe importance of effective data storage becomes increasingly important with the increasing integration of automation and to make the most of advances in data analysis and prediction.[\n\n1, \n\n2]\n\nComputationally-generated data can its own issues, particularly in terms of volume. As discussed previously, experimentally-generated and measured data comes in many different forms, from varied sources and can vary greatly in terms of its structure; for example, instrument readings, reaction conditions, experimental outcomes, and metadata about samples\n\nManaging the processing and storage of data from a lab setting is complex, even for relatively simple experimental workflows. Keeping the data structured, persistent, and easily accessible is crucial for reproducibility, analysis, and automation. The integration of automated processes might alleviate issues around structuring metadata and organising measurements, but can increase challenges in terms of the volume of data produced.","type":"content","url":"/workshop-files/lab-data-structure#data-storage-in-the-automated-lab","position":5},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl3":"Database systems for chemistry","lvl2":"Data storage in the automated lab"},"type":"lvl3","url":"/workshop-files/lab-data-structure#database-systems-for-chemistry","position":6},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl3":"Database systems for chemistry","lvl2":"Data storage in the automated lab"},"content":"Relational database management systems (RDBMS) offer one approach to structuring data storage: They store data in tables, where relationships between data points are explicitly defined. Unlike spreadsheets, which can contain duplicated or inconsistent information, relational databases enforce data integrity and reduce redundancy through normalisation.","type":"content","url":"/workshop-files/lab-data-structure#database-systems-for-chemistry","position":7},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"Advantages of Relational Databases","lvl3":"Database systems for chemistry","lvl2":"Data storage in the automated lab"},"type":"lvl4","url":"/workshop-files/lab-data-structure#advantages-of-relational-databases","position":8},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"Advantages of Relational Databases","lvl3":"Database systems for chemistry","lvl2":"Data storage in the automated lab"},"content":"Persistence Data is stored safely and remains available even after the program or system shuts down.\n\nStructure and integrity Data is organised into tables with clearly defined relationships, reducing errors and inconsistencies.\n\nScalability and Efficiency Efficient querying through SQL (Structured Query Language) allows rapid retrieval and filtering of large datasets.\n\nAccessibility Multiple users and systems can access and manipulate the data concurrently without conflicts.\n\nUsing a structured storage system like a relational database (db) enables a transition from unstructured data (e.g. scattered experimental records in spreadsheets) to a system where data is systematically stored, so that it remains accessible and useful for downstream automation and decision-making.","type":"content","url":"/workshop-files/lab-data-structure#advantages-of-relational-databases","position":9},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl3":"Aims","lvl2":"Data storage in the automated lab"},"type":"lvl3","url":"/workshop-files/lab-data-structure#aims","position":10},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl3":"Aims","lvl2":"Data storage in the automated lab"},"content":"","type":"content","url":"/workshop-files/lab-data-structure#aims","position":11},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"Part 1 SQL + Python","lvl3":"Aims","lvl2":"Data storage in the automated lab"},"type":"lvl4","url":"/workshop-files/lab-data-structure#part-1-sql-python","position":12},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"Part 1 SQL + Python","lvl3":"Aims","lvl2":"Data storage in the automated lab"},"content":"A sample of data from a set of lab experiments is available in the data directory as a flat file: cox_experiments.csv.\n\nUse the SQLAlchemy package to connect to a SQLite database\n\nRead a sample of data from an experimental lab study into a pandas dataframe\n\nProcess the data in pandas\n\nLoad the data into the SQLite database to store in a structured, persistent format","type":"content","url":"/workshop-files/lab-data-structure#part-1-sql-python","position":13},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"Part 2 Database structure","lvl3":"Aims","lvl2":"Data storage in the automated lab"},"type":"lvl4","url":"/workshop-files/lab-data-structure#part-2-database-structure","position":14},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"Part 2 Database structure","lvl3":"Aims","lvl2":"Data storage in the automated lab"},"content":"One of the key advantages of using a relational database system is that it imposes some degree of structure on the data. This can help with ensuring the data is complete and avoids redundancy. Simply storing available data in a single table is not the most effective way to achieve those goals. Designing the structure of the database using concepts such as data normalisation can help to avoid inconsistencies in the data and overly complex relationships between tables, for example.\n\nUsing the experiments table as a starting point\n\nConsider how the data could be organised using the concept of normalisation\n\nUsing the current columns, propose a design that achieves at least 3rd form normalisation\n\nList the tables required\n\nColumns in each table\n\nAny primary and foreign key relationships you think would be needed\n\nSuggest additional columns and/or tables you would add to improve the completeness or functionality of the database\n\nInvestigate ways to visualise the design a database, such as EER diagrams\n\nNote\n\nA notebook with some completed code for part 1 is available \n\nhere\n\n","type":"content","url":"/workshop-files/lab-data-structure#part-2-database-structure","position":15},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl2":"Part 1 SQL + Python"},"type":"lvl2","url":"/workshop-files/lab-data-structure#part-1-sql-python-1","position":16},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl2":"Part 1 SQL + Python"},"content":"","type":"content","url":"/workshop-files/lab-data-structure#part-1-sql-python-1","position":17},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl3":"Structured Query Language (SQL)","lvl2":"Part 1 SQL + Python"},"type":"lvl3","url":"/workshop-files/lab-data-structure#structured-query-language-sql","position":18},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl3":"Structured Query Language (SQL)","lvl2":"Part 1 SQL + Python"},"content":"Structured Query Language (SQL) is a language used to query and manipulate databases. In CHEM501, you used a \n\nSQLite database to store data in a structured form. You were able to connect to the database using python and execute queries in raw SQL, and this included adding data from a pandas DataFrame to a table in an SQL database.","type":"content","url":"/workshop-files/lab-data-structure#structured-query-language-sql","position":19},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"SQL and Python","lvl3":"Structured Query Language (SQL)","lvl2":"Part 1 SQL + Python"},"type":"lvl4","url":"/workshop-files/lab-data-structure#sql-and-python","position":20},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"SQL and Python","lvl3":"Structured Query Language (SQL)","lvl2":"Part 1 SQL + Python"},"content":"Being able to connect Python with a database such as a SQL db, offers the advantages of being able carry out data processing, analysis and modelling using the wide range of Python libraries, standardising data processes before storing the processed and analysed data in a persistent, structured format that is accessible to multiple users.\n\nAutomation and integration with data sources and workflows\n\nPython can enable integration with lab instruments, data pipelines, and machine learning models.\n\nThis can faciliate automated data entry, validation, and retrieval without manual intervention.\n\nData processing and analysis\n\nPython provides a wide variety of libraries (e.g. NumPy, Pandas, SciPy, etc.) for analysing and visualising stored data.\n\nData can be accessed directly from the database, processed, and piped into further optimisation or scientific models.\n\nReproducibility & Consistency\n\nDeveloping Python scripts for data processing can ensure that it is standardised and reproducible across experiments.","type":"content","url":"/workshop-files/lab-data-structure#sql-and-python","position":21},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"SQLAlchemy","lvl3":"Structured Query Language (SQL)","lvl2":"Part 1 SQL + Python"},"type":"lvl4","url":"/workshop-files/lab-data-structure#sqlalchemy","position":22},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"SQLAlchemy","lvl3":"Structured Query Language (SQL)","lvl2":"Part 1 SQL + Python"},"content":"SQLalchemy is a package that lets you interact with a SQL database using python. It provides an abstraction layer over the raw SQL: It distances you from SQL, so you do not need to write raw SQL. This can make database interactions easier and more ‘Pythonic’.\n\nIt also means that it is more straightforward to change the underlying SQL database (e.g. from SQLite to mySQL or PostgreSQL) if you decide to later.\n\nSQLAlchemy manages connections to the database, so you do not need to worry about closing connections. The abstraction away from raw SQL queries means data validation can be built in and can prevent security issues like SQL injection.\n\nHere is an \n\nSQLalchemy with sqlite tutorial  that will help as you work through the notebook. The main points you will see in this notebook focus on getting started quickly using familiar data structures in Python, by showing how straightforward working with pandas dataframes and a sql database can be.\n\n# TODO: read the data into a dataframe\n\n\n\n\nPandas can output a dataframe directly to a sql table using the dataframe’s \n\nto_sql method and a SQLAlchemy connection.","type":"content","url":"/workshop-files/lab-data-structure#sqlalchemy","position":23},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"Tasks","lvl3":"Structured Query Language (SQL)","lvl2":"Part 1 SQL + Python"},"type":"lvl4","url":"/workshop-files/lab-data-structure#tasks","position":24},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"Tasks","lvl3":"Structured Query Language (SQL)","lvl2":"Part 1 SQL + Python"},"content":"Create an engine to connect to the sqlite database\n\nSQLAlchemy’s create_engine method makes an engine object representing a way to connect to a db. If the database does not already exist, it will be created.\n\nConnect to the database\n\nThe engine object is then instructed to make the connection using the connect function.\n\nCreate an object for the metadata\n\nInformation about the structure of the database can be accessed or manipulated via a MetaData object.\n\nOutput the contents of the dataframe to a database table\n\nUse the pandas dataframe to_sql method to load the dataframe contents into a table in the SQL db. The if_exists argument specifies what happens if the table already exists. It returns the number of rows added to the database.\n\n# TODO: create a connection to the database\n# Call the engine and connection objects engine and connection\n\n# Create an object to hold the table metadata\n\n\n\n# If you want to list all existing tables in the database\n# inspector = inspect(engine)\n# inspector.get_table_names()\n\n\n\n# TODO: Load the contents of the experiment DataFrame into a table called test_experiments in the database\n\n\n\n\n\nYou can check the columns of the new table in the SQL database by reading the table and printing their name and the data types:\n\n# This code assumes the SQL db table you created is called test_experiments\n# It creates a table object that you can use to query the database and uses\n# the metadata object to reflect the table from the db so you can see the \n# existing structure of the table and columns\n\n# table = db.Table('test_experiments', metadata, autoload_with=engine)\n# for col in table.columns:\n#     print(col, col.type)\n\n\n\nThe pandas to_csv method uses the data types for the columns to set the column types in the SQL table. The pandas dtypes need to be correct to ensure the correct structure of the db table is created.\n\n# TODO: Check the dtypes of the pandas DataFrame columns and compare them to the dtypes of the columns in the SQL table\n# experiment_df.info()\n\n\n\n\nThe date column of the dataframe, is currently stored as a string, so should be converted to a datetime.\n\ntemperature is an int64, but using a float would better allow for higher precision in future data.\n\n# TODO: change the date column to datetime and temperature to float\n\n\n\n\nIt is possible to change the data types of the table, but as the table has very little data, we can just delete/replace it to start again.\n\nHere, any the existing table will be replaced by the to_sql function, so you do not have to explicitly delete (drop) the table. You do need to clear the MetaData object so that it does not impose the previous structure of the table.\n\n# The line below explicitly deletes the table from the database.\n# table.drop(engine)\n\n# TODO: use the clear method to remove the previous table metadata from the metadata object\n# This allows the table to be redefined with the new datatypes\n\n\n\n\n# TODO: Check that data types have been changed in the SQL table\n\n\n\n\nAt this point, you have created a SQLite database and a table within it to store the sample experimental data from a csv file.\n\nYou have imposed a specific structure on the data so that specific data types are required for some columns.\n\nAs well as loading data directly from a pandas dataframe into an sql database table, you can do the opposite process: Read the contents of the db table into a dataframe to check that the data does actually persist in the database.\n\n# TODO: Read data from the SQLite database into a dataframe. Try the read_sql_table method or use\n# a query to read the data.\n\n\n\n\n# or use a query to read the data\n# query = \"SELECT * FROM test_experiments\"\n# result_df = pd.read_sql(query, engine)\n# result_df\n\n# This gives you an idea of how to filter the data returned from the database, e.g.\n# query = \"SELECT * FROM test_experiments WHERE temperature > 20\"\n# result_df = pd.read_sql(query, engine)\n# result_df\n\n\n\n\n\n","type":"content","url":"/workshop-files/lab-data-structure#tasks","position":25},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl3":"Other flavours of SQL","lvl2":"Part 1 SQL + Python"},"type":"lvl3","url":"/workshop-files/lab-data-structure#other-flavours-of-sql","position":26},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl3":"Other flavours of SQL","lvl2":"Part 1 SQL + Python"},"content":"SQLite is an excellent option for local storage, prototyping, and simple applications. It is lightweight, does not require any setup, so is perfect for single-user applications or embedded systems.\n\nMore fully-featured versions of SQL are available, both commercial (e.g. Microsoft SQL Server, Oracle DB, IBM Db2) and open source (PostgreSQL, MySQL, MariaDB), in addition to versions for cloud and big data.\n\nPostgreSQL and \n\nMySQL are both widely-used open source, true server-based databases designed for multi-user environments. They support concurrency, security, and large-scale operations.\n\nIf you have time or you want to try out a real SQL database system, you can install MySQL and connect to it using SQLAlchemy. The \n\nMySQL Community Edition is free to use under the GPL licence. It has a very user-friendly user interface \n\nMySQL Workbench that makes it easy to visualise the database contents and structure.\n\nTo use SQLAlchemy with MySQL, you will also need to install a python driver such as \n\nMySql Connector (available on pip or conda as \n\nmysql​-connector​-python) or \n\nPyMySQL.\n\nA major advantage of using SQLAlchemy is that it makes swapping the underlying SQL db system relatively pain-free (usually). The code below shows how to create a new connection to a MySQL database and\n\n# DB password is stored in a separate file - add this file to gitignore if you are using git\n\n# from db_cred import mspwd\n\n\n\n# # only needed for mySQL\n# connection_string = f\"mysql+mysqlconnector://root:{mspwd}@127.0.0.1:3306/chem_test\"\n# engine = db.create_engine(connection_string) # add for logging db actions: echo=True)\n# metadata = db.MetaData()\n\n\n\n# experiment_df\n\n\n\n# experiment_df.to_sql(\"experiments\", engine, if_exists='append', index=False)\n\n\n\n\n","type":"content","url":"/workshop-files/lab-data-structure#other-flavours-of-sql","position":27},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl2":"Part 2 Database structure"},"type":"lvl2","url":"/workshop-files/lab-data-structure#part-2-database-structure-1","position":28},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl2":"Part 2 Database structure"},"content":"","type":"content","url":"/workshop-files/lab-data-structure#part-2-database-structure-1","position":29},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl3":"Brief introduction to data normalisation","lvl2":"Part 2 Database structure"},"type":"lvl3","url":"/workshop-files/lab-data-structure#brief-introduction-to-data-normalisation","position":30},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl3":"Brief introduction to data normalisation","lvl2":"Part 2 Database structure"},"content":"See Also\n\nDataCamp - Normalization in SQL (1NF - 5NF): A Beginner’s Guide\n\nPopSQL - Normalization in SQL DBMS\n\nWhen designing a relational database, a key goal is eliminating redundancy and ensuring data integrity. This process is called normalisation. It involves organising data into multiple related tables instead of storing everything in a single, unstructured table.","type":"content","url":"/workshop-files/lab-data-structure#brief-introduction-to-data-normalisation","position":31},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"Why normalise data?","lvl3":"Brief introduction to data normalisation","lvl2":"Part 2 Database structure"},"type":"lvl4","url":"/workshop-files/lab-data-structure#why-normalise-data","position":32},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"Why normalise data?","lvl3":"Brief introduction to data normalisation","lvl2":"Part 2 Database structure"},"content":"Reduces redundancy: Avoids storing the same information multiple times (e.g., CAS numbers for chemicals).Improves consistency: Changes in one place update automatically across the database.Enhances scalability: Structured data is easier to query, update, and expand.","type":"content","url":"/workshop-files/lab-data-structure#why-normalise-data","position":33},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"The experiment data example","lvl3":"Brief introduction to data normalisation","lvl2":"Part 2 Database structure"},"type":"lvl4","url":"/workshop-files/lab-data-structure#the-experiment-data-example","position":34},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"The experiment data example","lvl3":"Brief introduction to data normalisation","lvl2":"Part 2 Database structure"},"content":"The experimental data you looked at in the earlier part has several issues in the way it is currently structured.\nIt repeats chemical names and CAS numbers** for coformers, and stores file paths as plain text, which can make querying difficult. A normalised database would separate:\n\nExperiments - linked to unique chemicals, solvents, and  characterisation data.\n\nChemicals - each chemical has a unique CAS number, reducing repetition.\n\nCharacterisation Data - storing file paths separately allows easy retrieval.","type":"content","url":"/workshop-files/lab-data-structure#the-experiment-data-example","position":35},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"For you to consider","lvl3":"Brief introduction to data normalisation","lvl2":"Part 2 Database structure"},"type":"lvl4","url":"/workshop-files/lab-data-structure#for-you-to-consider","position":36},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"For you to consider","lvl3":"Brief introduction to data normalisation","lvl2":"Part 2 Database structure"},"content":"What issues might arise if the same chemical has multiple CAS numbers in different rows?\n\nHow could we store characterisation data (NMR, PXRD) in a structured way instead of file paths in a single table?\n\nHow can we design a system where updating or correcting a chemical’s CAS number does not require changing existing experiment records?\n\nWhat other information could be included in the database that would make the current data more complete or useful?\n\nSee Also\n\nThis post introduces several common schema for database design.\n\n\n6 Database Schema Designs and How to Use Them\n\n","type":"content","url":"/workshop-files/lab-data-structure#for-you-to-consider","position":37},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"Design a normalised structure for the example data","lvl3":"Brief introduction to data normalisation","lvl2":"Part 2 Database structure"},"type":"lvl4","url":"/workshop-files/lab-data-structure#design-a-normalised-structure-for-the-example-data","position":38},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"Design a normalised structure for the example data","lvl3":"Brief introduction to data normalisation","lvl2":"Part 2 Database structure"},"content":"The experimental dataset you have looked at is a flat file that stores all information in a single table.\n\nSuggest a new set of tables to accommodate the data\n\nDecide what columns (and think about data types) the tables need\n\nThe structure should normalise the data up to at least normal form 3\n\nIdentify primary keys and/or foreign keys that can be used to embed relationships between tables\n\nSketch or explore ways to visualise the database structure, such as \n\nER diagrams\n\nDBeaver might be an option for generating an ER diagram for an SQLite database.\n\nMySQL Workbench has a built in \n\nER tool if you have tried out an MySQL db. If you have an existing MySQL db, go to Database > Reverse engineer to get the ER for an existing db.","type":"content","url":"/workshop-files/lab-data-structure#design-a-normalised-structure-for-the-example-data","position":39},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl5":"If you have time","lvl4":"Design a normalised structure for the example data","lvl3":"Brief introduction to data normalisation","lvl2":"Part 2 Database structure"},"type":"lvl5","url":"/workshop-files/lab-data-structure#if-you-have-time","position":40},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl5":"If you have time","lvl4":"Design a normalised structure for the example data","lvl3":"Brief introduction to data normalisation","lvl2":"Part 2 Database structure"},"content":"Create a new SQLite or MySQL database with the normalised structure\n\nYou can use raw SQL with sqlite or SQLAlchemy using Python\n\nOR if you have MySQL installed, create the database in MySQL Workbench and generate the structure\n\n","type":"content","url":"/workshop-files/lab-data-structure#if-you-have-time","position":41},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl3":"Summary","lvl2":"Part 2 Database structure"},"type":"lvl3","url":"/workshop-files/lab-data-structure#summary","position":42},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl3":"Summary","lvl2":"Part 2 Database structure"},"content":"As you worked through this notebook, you have\n\nused the SQLAlchemy package to interact with a SQLite database\n\nloaded data from a pandas dataframe directly into a SQL database\n\nread data from an SQL database table into a dataframe\n\ngained awareness of core principles of database design\n\nconsidered an example of data normalisation based on a typical flat file containing experimental lab information","type":"content","url":"/workshop-files/lab-data-structure#summary","position":43},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"For you to think about","lvl3":"Summary","lvl2":"Part 2 Database structure"},"type":"lvl4","url":"/workshop-files/lab-data-structure#for-you-to-think-about","position":44},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"For you to think about","lvl3":"Summary","lvl2":"Part 2 Database structure"},"content":"In your project, what are the sources of the data you will be working with?\n\nHow will they be processed before you store them?\n\nHow will they be stored? As flat files distributed across locations/servers or consolidated somehow?\n\nHow will you deal with metadata and connecting data from different sources, experiments, calculations?\n\nIf you were designing a data management system, where would you start? What kind of structure would be appropriate?\n\nHow could this improve the quality of the data (e.g. reduce errors, redundancy) or improve accessibility in the context of your project and/or research group?\n\n","type":"content","url":"/workshop-files/lab-data-structure#for-you-to-think-about","position":45},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"References","lvl3":"Summary","lvl2":"Part 2 Database structure"},"type":"lvl4","url":"/workshop-files/lab-data-structure#references","position":46},{"hierarchy":{"lvl1":"Structured data and digital chemistry","lvl4":"References","lvl3":"Summary","lvl2":"Part 2 Database structure"},"content":"R. Duke, V. Bhat and C. Risko, Data storage architectures to accelerate chemical discovery: data accessibility for individual laboratories and the community, Chem. Sci., 2022, 13, 13646–13656.\n\n1 K. M. Jablonka, L. Patiny and B. Smit, Making the collective knowledge of chemistry open and machine actionable, Nat. Chem., 2022, 14, 365–376.","type":"content","url":"/workshop-files/lab-data-structure#references","position":47}]}