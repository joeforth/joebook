{"version":3,"kind":"Notebook","sha256":"2e5ab232048db96734ee9e7f1fc3b478e9b2e84d297708a5ff68f81503ad838c","slug":"ml-intro.ml-demo","location":"/3-ml_intro/ML_demo.ipynb","dependencies":[],"frontmatter":{"title":"Notebook - ML for thermochemical data prediction","content_includes_title":true,"kernelspec":{"name":"python3","display_name":"chem502","language":"python"},"authors":[{"id":"Sam Chong","name":"Sam Chong"},{"id":"Joe Forth","name":"Joe Forth"}],"github":"https://github.com/joeforth/chem502_book","numbering":{"title":{"offset":1}},"exports":[{"format":"ipynb","filename":"ML_demo.ipynb","url":"/ML_demo-cfb3fea2668d37e5e928ed518568fd7a.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\nfrom rdkit import Chem\nfrom rdkit.Chem import (\n                        PandasTools,\n                        Descriptors\n                        )\n\nfrom rdkit.Chem.Draw import IPythonConsole","visibility":"hide","key":"TXM4h1wDyN"},{"type":"outputs","id":"JN62rfUVUJYD5mx-DfFdQ","children":[],"visibility":"show","key":"A7qqW9WLHV"}],"visibility":"show","key":"RrogA5ojby"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":1,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Notebook - ML for thermochemical data prediction","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OB3zvGuEn8"}],"identifier":"notebook-ml-for-thermochemical-data-prediction","label":"Notebook - ML for thermochemical data prediction","html_id":"notebook-ml-for-thermochemical-data-prediction","implicit":true,"key":"TNr9D7aXBa"}],"key":"kEnJp2ocmh"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We can use the data you worked with for the EDA workshop to demonstrate how fit a simple ML model to predict the boiling point of organic compounds from a small set of descriptors.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Z0wod71aLT"}],"key":"Zg75sQmGmJ"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This is an example of","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ZwglEjWZOm"}],"key":"monktWE1Tb"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"strong","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Supervised learning","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"GBkxep7b6N"}],"key":"Hya9tAvsb9"},{"type":"text","value":" as applied to a ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"Rsw4mpmQfB"},{"type":"strong","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"regression","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"g0zfnFkWGz"}],"key":"z1pMhjgoYi"},{"type":"text","value":" task.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"OoIE49JEEa"}],"key":"zPBCQyhPN8"},{"type":"definitionList","position":{"start":{"line":9,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"definitionTerm","position":{"start":{"line":9,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Supervised learning","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"OAjoZAG92x"}],"key":"i7AjRlnmj0"},{"type":"definitionDescription","position":{"start":{"line":10,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"The model learns from data for which a label (known value or category) is associated with each input (set of feature values)","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"WBKFIE5Va7"}],"key":"lVMxMvZDPY"},{"type":"definitionTerm","position":{"start":{"line":12,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Regression task","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"IZURRne8gT"}],"key":"F2rNOwquJL"},{"type":"definitionDescription","position":{"start":{"line":12,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Model predicts a continuous numerical value based on the input features","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"IlrZrgZSiF"}],"key":"G3AIEAApxb"}],"key":"zYn5NsSVdY"}],"key":"TEKuoBnLhZ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"bp_df = pd.read_csv(\"data/alcohol_acid_phys_data_cleaned.csv\")\nbp_df","key":"IcHLrSXGvQ"},{"type":"outputs","id":"H3xZNTQ1YSK1NcLW1NyO4","children":[],"key":"csHOESbePy"}],"key":"wvAPFicoIH"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The data is read in from a csv file of the data cleaned in the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VkzscuFxNF"},{"type":"link","url":"/workshop-files/eda-workshop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"EDA workshop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YzrFyPwnXb"}],"urlSource":"../workshop_files/eda_workshop","dataUrl":"/workshop-files.eda-workshop.json","internal":true,"protocol":"file","key":"Xh5bngyh8J"},{"type":"text","value":", so should be prepared, but we can quickly check.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Q0b0QSxdqv"}],"key":"zrALPQ3QZy"}],"key":"nvemr1QRBL"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Check for missing values\nbp_df.isna().sum()","key":"C8fSXAJeUt"},{"type":"outputs","id":"25EnyukYAy1JIFZj6VXg9","children":[],"key":"XJ4XDIkeVG"}],"key":"aZTcrOdlUi"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Check data types\nbp_df.info()","key":"Cc99diuI5i"},{"type":"outputs","id":"oNx1mPRlMU7LeMBpfPIq2","children":[],"key":"mIxxZ4v1fq"}],"key":"AwuHD1gbPF"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The numerical types are as expected. The category type for the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xFXEHAtved"},{"type":"inlineCode","value":"class","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OgWXeYnvJT"},{"type":"text","value":" column has not been automatically recognised, understandably. For now we will not be working with that column, so will leave as is.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AGo1mByG13"}],"key":"rNYqzlv7Jp"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"We will start by dropping any rows without a SMILES string and also the melting point column as we are going to try to predict the boiling point.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"csG9zvyTPo"}],"key":"RMM4BKQXJ0"}],"key":"jTrMpzz1Gz"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Drop rows with missing SMILES string\nbp_df = bp_df[bp_df[\"SMILES\"] != \"not found\"].reset_index(drop=True)\n\nbp_df = bp_df.drop(columns=[\"mp / dC\"])\nbp_df","key":"XqX8qjMBpg"},{"type":"outputs","id":"y2K64iF2sAL4jX3X1LTE0","children":[],"key":"f9LV9wjEEN"}],"key":"TPVypcQ1z4"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Add RDKit molecules to the dataframe\nPandasTools.AddMoleculeColumnToFrame(bp_df, smilesCol=\"SMILES\")\nbp_df","key":"KDedYh3Res"},{"type":"outputs","id":"mIT6p0eL4jpp89Qo7mx0w","children":[],"key":"SyHqFr8HUw"}],"key":"kFFIGryipI"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"# Adapted from https://greglandrum.github.io/rdkit-blog/posts/2022-12-23-descriptor-tutorial.html\n\ndef getMolDescriptors(mol, descriptor_list=None, missingVal=None):\n    ''' calculate the full list of descriptors for a molecule\n    \n        missingVal is used if the descriptor cannot be calculated\n    '''\n    res = {}\n    if not(descriptor_list):\n        descriptors = Descriptors._descList\n    # TODO: Add else clause to handle a list numbers corresponding to the descriptor indices\n    else:\n        descriptors = [Descriptors._descList[idx] for idx in descriptor_list]\n\n    for nm,fn in descriptors:\n        # some of the descriptor fucntions can throw errors if they fail, catch those here:\n        try:\n            val = fn(mol)\n        except:\n            # print the error message:\n            import traceback\n            traceback.print_exc()\n            # and set the descriptor value to whatever missingVal is\n            val = missingVal\n        res[nm] = val\n    return res","visibility":"hide","key":"M6eGEY77S3"},{"type":"outputs","id":"ArCkWmtWkOJoKNufaPBbI","children":[],"visibility":"show","key":"nGGIv51FiY"}],"visibility":"show","key":"JZ0MnnjgLV"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# These are the descriptors selected to calculate for the molecules.\n# 118 NumHAcceptors\n# 119 NumHDonors\n# 27 BalabanJ - a topological descriptor expressing molecular connectivity and branching\n# 28 BertzCT - a topological complexity index\n# 83 TPSA - total polar surface area\n\ndescriptor_list = [118, 119, 27, 28, 83]\n\n\ncalc_descriptors = [getMolDescriptors(mol, descriptor_list=descriptor_list) for mol in bp_df[\"ROMol\"]]\n\n# Create a dataframe from the calculated descriptors\ndescriptor_df = pd.DataFrame(calc_descriptors)\n\n# Add the descriptors to the dataframe as new columns\nbp_df = pd.concat([bp_df, descriptor_df], axis=1)\nbp_df","key":"OBbLfVOLjB"},{"type":"outputs","id":"hTHkBaHiGq1a3Mu95whtC","children":[],"key":"hTBPcyvoHB"}],"key":"b0yY2lo3WQ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# To remind us of the relationship between the variables, we can plot a pairplot and heatmap\n# Pairplot\nsns.pairplot(bp_df, hue=\"Class\", diag_kind=\"kde\",  palette=\"viridis\")\nplt.show()","key":"sw9Bj373rD"},{"type":"outputs","id":"LDsI3MNd3W5QskEjoTvPE","children":[],"key":"UeqBrLxlNq"}],"key":"L0X6hM9LFX"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"corr = bp_df.drop(columns=[\"Class\", \"IUPAC name\", \"SMILES\", \"ROMol\"]).corr()\n\nsns.heatmap(corr, annot=True)\nplt.show()","key":"p45P7k90iz"},{"type":"outputs","id":"7dGpZjq_sMF9BsQXshhE2","children":[],"key":"MLmzZLHiCA"}],"key":"hDEC4wZuo8"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# corr_abs = bp_df.drop(columns=[\"Class\", \"IUPAC name\", \"SMILES\", \"ROMol\"]).corr().abs()\n# upper = corr_abs.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n# upper","key":"eUSM8YTVZy"},{"type":"outputs","id":"L7pBU2AT65cv6vM0fgcG9","children":[],"key":"ewcl3bw8oT"}],"key":"VmNe5Cu43Z"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"This is all looking rather busy - it would be better to look at subsets of the features.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QH9YHx20HG"}],"key":"iCbGxkBRFb"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"What we can see is:","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ViGhuWjDH2"}],"key":"iCGiAxExNY"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"The feature most strongly correlated with the target variable is the molecular weight.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"vksxdczeFG"}],"key":"BJieZLzPwS"}],"key":"AmPdibbLUD"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Molecular weight is very strongly correlated with number of carbons and hydrogens","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"sGYnUTfNBk"}],"key":"xR0AHRSa1I"}],"key":"aKjiZiloez"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Most of the other features are only moderately or weakly correlated to the target variable.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"sJ9eCBAyhg"}],"key":"PQX7juIFWI"}],"key":"BQVSgxxk7P"}],"key":"UCpEGFqCHh"}],"key":"UtD3L3t3tn"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# A closer look at the correlation between the boiling point and the molecular weight\nsns.regplot(data=bp_df, x=\"Molweight g/mol\", y=\"bp / dC\",  fit_reg=True,  ci=None)\nplt.show()","key":"uAiPSrK9u6"},{"type":"outputs","id":"JZ2muJ6aoe-Noiuo4guOJ","children":[],"key":"nWXsW6r3Hr"}],"key":"xW5bpp4NS6"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"For the moment, we will drop the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YK0bxWxT76"},{"type":"inlineCode","value":"#C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JpibPybmYD"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ufjDI2ePbf"},{"type":"inlineCode","value":"#H","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gfQBj0plhv"},{"type":"text","value":" columns. We will see that analysing the initial model can help tell us about the importance of the features.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KKowfcfZ6n"}],"key":"tyMTbbwLbW"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"We will also drop the non-numerical features for this task.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"HCxN7Icsf7"}],"key":"Kww9px2Ixi"}],"key":"kuxHMONzOF"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"cols_drop = [\"#C\", \"#H\", \"IUPAC name\", \"SMILES\", \"ROMol\", \"Class\"]\n\nprep_df = bp_df.drop(columns=cols_drop)\nprep_df","key":"ed8oH2yd6T"},{"type":"outputs","id":"UhiMEGwtCAsrBKwUH5TlO","children":[],"key":"CKftA0QBxt"}],"key":"CHYp6X5Ifa"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler","key":"kGT2or5bLt"},{"type":"outputs","id":"5WAhawc4iaMtZGCO29tur","children":[],"key":"GwbptBSyuY"}],"key":"scrH4EccOA"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# By convention, the target variable is denoted by y and the features are denoted by X\n\ny_full = prep_df[\"bp / dC\"]\nX_full = prep_df.drop(columns=[\"bp / dC\"])","key":"BHqLnWxrDG"},{"type":"outputs","id":"eYUzEn_NgvmcROzmwhQyv","children":[],"key":"MVLxNfvMa9"}],"key":"m8qUzwVye9"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# train_test_split shuffles the data by default and splits it into training and testing sets. The \n# proportion of the data that is used for testing is determined by the test_size parameter. Here, \n# we are using 80 % of the data for training and 20% for the test set. \n# The random_state parameter is used to set the seed for the random number generator so that the \n# results are reproducible.\n\nX_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42)","key":"X4VHU31eHb"},{"type":"outputs","id":"x2R6lRCCY-4CXVD7BVGAc","children":[],"key":"t86PNF3tQs"}],"key":"Oxb4nk8UPN"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Check the size of the training and testing sets\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","key":"qXywPuuOSp"},{"type":"outputs","id":"1US7SJEo8UgckUagZakcc","children":[],"key":"DDbsmsHrmE"}],"key":"B9glBFK2tZ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"link","url":"https://scikit-learn.org/stable/index.html","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"Scikit-learn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MKNuBHP3OQ"}],"urlSource":"https://scikit-learn.org/stable/index.html","key":"sHVN3kl2ec"},{"type":"text","value":" makes many models available via a consistent interface.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pUWoWO6xVo"}],"key":"HMeJbCZqcA"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"We are going to use a linear regression model for this task.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ZIY2YgXj70"}],"key":"X74aPmQOTk"}],"key":"qUD5kWkvPR"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from sklearn import linear_model","key":"hMrMX2SoR2"},{"type":"outputs","id":"R-26jhVomJxT0yUc6UkVd","children":[],"key":"Czlh6V3pao"}],"key":"g5fQPxps73"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Create a linear regression model\nreg = linear_model.LinearRegression()\n\n# Fit the model to the training data\nreg.fit(X_train, y_train)","key":"ahrKGxqoQC"},{"type":"outputs","id":"UAKklmqB_OA9NVSLHCP4x","children":[],"key":"UiqMq23FxO"}],"key":"P0BvNpk7FO"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Predict the boiling points of the test set\ny_pred = reg.predict(X_test)\n","key":"Iwu4xmdyLx"},{"type":"outputs","id":"j8X4ZjwZ8WxpG5eFMdN2B","children":[],"key":"EMGbrU0m7k"}],"key":"qc6Y4aAPdo"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We can plot the boiling points that the model predicted for the test set against the known true values to see how good a job the model makes of the predictions","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YAzHEfke2o"}],"key":"lAQu9iWaBj"}],"key":"M699aKh79r"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"\nplt.scatter(y_test, y_pred)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '-', lw=1, color=\"red\")\nplt.xlabel(\"True Values [bp / dC]\")\nplt.ylabel(\"Predictions [bp / dC]\")\nplt.show()","key":"bBGrzrGNZg"},{"type":"outputs","id":"Eo8a5npwvoTRGK2jB2DBY","children":[],"key":"ZGCcDeEvGI"}],"key":"Xwzo8Qj8Gt"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# The r^2 value is a measure of how well the model fits the data. It ranges from 0 to 1, \n# with 1 indicating a perfect fit.\nr2 = reg.score(X_test, y_test)\nr2","key":"yXca7RxZoZ"},{"type":"outputs","id":"TJp3DuyNtfmR6xNNTL8uN","children":[],"key":"U2hN3psk1d"}],"key":"Ai71SoAICV"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"If we look back, we can see that the Pearson correlation coefficient for the relationship between molecular weight and boiling point was 0.84, so the model predicts more accurately than using the molecular weight alone.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FCD5AbPWew"}],"key":"JimYG8fPtW"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"The model coefficients tell us about the weighting of the features used by the fitted model.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"QDP3HOqJP9"}],"key":"EeXgiBzdAI"}],"key":"Z9FpWKbsob"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print(X_full.columns)\nreg.coef_","key":"XOeYLd7NgJ"},{"type":"outputs","id":"G9NLe8vqgChuZjmC3sB91","children":[],"key":"VXj1d01ji2"}],"key":"r83AX8kaKr"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"However, because the magnitude of the features’ values are on different scales, the coefficients also incorporate the different scales.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cn63zdmwMi"}],"key":"XYKnxAPCMr"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"A scaler can be used to transform the features to a consistent scale. Here’s we’ll use a ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ozNQDFKWSb"},{"type":"link","url":"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"inlineCode","value":"MinMaxScaler","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"MHcnn21LS2"}],"urlSource":"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html","key":"f7XKhvXnQ5"},{"type":"text","value":" to transform the features to have a scale between 0 and 1.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"QEd70td9l9"}],"key":"BRuePy0MeQ"}],"key":"YJqTT8zSFO"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Split the scaled data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42)\n\n# create a min/max scaler\nscaler = MinMaxScaler()\n\n# fit the scaler to the training data\nscaler.fit(X_train)\n\n# transform the training and testing data separately\nscaled_X_train = scaler.transform(X_train)\nscaled_X_test = scaler.transform(X_test)","key":"VivI7iFvkw"},{"type":"outputs","id":"Um5gEQU_zZGpoLBZwKECW","children":[],"key":"vr7ZydfvHi"}],"key":"T4GHMeFAHq"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Create a linear regression model\nreg = linear_model.LinearRegression()\n\n# Fit the model to the training data\nreg.fit(scaled_X_train, y_train)\n\n# Predict the boiling points of the test set\ny_pred = reg.predict(scaled_X_test)","key":"xCYKdCVKYM"},{"type":"outputs","id":"l0d68-pFC2xMBqUhcMwR9","children":[],"key":"Xtj8JtYWfh"}],"key":"uKVSgux7RX"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plt.scatter(y_test, y_pred)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '-', lw=1, color=\"red\")\nplt.xlabel(\"True Values [bp / dC]\")\nplt.ylabel(\"Predictions [bp / dC]\")\nplt.show()","key":"YPHnYrAE9e"},{"type":"outputs","id":"oJ7z00cIaGou0aVXA6lte","children":[],"key":"wbCAYSnOu5"}],"key":"X1wc3OtiTW"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# calculate the R^2 score\nr2 = reg.score(scaled_X_test, y_test)\nr2","key":"sqUr14edMV"},{"type":"outputs","id":"F5eV2foCkCnZ-Llxyhtpk","children":[],"key":"xl6s3khoZ7"}],"key":"UFjLntcveL"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The model’s predictions look the same as before, but we can now look at the coefficients.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"x8YflzdYym"}],"key":"eWZdGoG54p"}],"key":"zghshYKx4U"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print(X_full.columns)\nreg.coef_","key":"YSNa4mzBPA"},{"type":"outputs","id":"_yMtzf3qKBaS8RD-OnY_G","children":[],"key":"mvE3VIR2C0"}],"key":"bCcqCw9aSu"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We can now see that the coefficients which represent the weights of the features in the fitted model indicate that molecular weight - as expected - and density are contributing most strongly to the model","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JJTvRxkZq2"}],"key":"C2docw9bC2"}],"key":"mIAAMj6jh7"}],"key":"OxeQ60wyh3"},"references":{"cite":{"order":[],"data":{}}}}